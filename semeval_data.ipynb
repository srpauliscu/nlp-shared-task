{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrBZoUYUfWlo9tIfu/2Nyh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "b3W2ywtdABiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Format\n",
        "\n",
        "**Data:** `arguments-training/validation/testing.tsv`\n",
        "(5220 arguments)\n",
        "- Argument ID\n",
        "- Conclusion \n",
        "- Stance (e.g., in favor, against)\n",
        "- Premise (justification for conclusion)\n",
        "\n",
        "**Labels:** `labels-training/validation/testing.tsv` \n",
        "(20 binary value labels per argument)\n",
        "- Argument ID\n",
        "- Self-direction: thought\n",
        "- Self-direction: action\n",
        "- Stimulation\n",
        "- Hedonism\n",
        "- Achievement\n",
        "- Power: dominance\n",
        "- Power: resources\n",
        "- Face\n",
        "- Security: personal\n",
        "- Security: societal\n",
        "- Tradition\n",
        "- Conformity: rules\n",
        "- Conformity: interpersonal\n",
        "- Humility\n",
        "- Benevolence: caring\n",
        "- Benevolence: dependability\n",
        "- Universalism: concern\n",
        "- Universalism: nature\n",
        "- Universalism: tolerance\n",
        "- Universalism: objectivity\n",
        "\n",
        "**Access:** https://doi.org/10.5281/zenodo.6814563"
      ],
      "metadata": {
        "id": "X2Bgm5JhCPPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "4dEBasNDF6Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear working environment\n",
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHAtlPPEIJpZ",
        "outputId": "503cc971-6b07-460f-dfa6-354140033627"
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JXuWLoltF_wR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841d4618-ed40-41ee-f016-46f8d5f72b2c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lORatXxJmjjR",
        "outputId": "1a249c0e-8ce4-4c80-d8fe-bad9fc0c0945"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 43.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7MiENnaFF5ra"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "IYMeIf-cEz4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training arguments\n",
        "train_args_df = pd.read_csv('/content/drive/MyDrive/csci5832_project/data/arguments-training.tsv', sep='\\t')\n",
        "# view structure\n",
        "train_args_df.info()"
      ],
      "metadata": {
        "id": "H9k2Hl9REzjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6d58c7-dde7-451e-8f30-8fa8b7e57f28"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 163.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training labels\n",
        "train_labs_df = pd.read_csv('/content/drive/MyDrive/csci5832_project/data/labels-training.tsv', sep='\\t')\n",
        "# view structure\n",
        "train_labs_df.info()"
      ],
      "metadata": {
        "id": "YW-jlIILANgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14254fa6-8c04-45ea-985f-bec51a847012"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 21 columns):\n",
            " #   Column                      Non-Null Count  Dtype \n",
            "---  ------                      --------------  ----- \n",
            " 0   Argument ID                 5220 non-null   object\n",
            " 1   Self-direction: thought     5220 non-null   int64 \n",
            " 2   Self-direction: action      5220 non-null   int64 \n",
            " 3   Stimulation                 5220 non-null   int64 \n",
            " 4   Hedonism                    5220 non-null   int64 \n",
            " 5   Achievement                 5220 non-null   int64 \n",
            " 6   Power: dominance            5220 non-null   int64 \n",
            " 7   Power: resources            5220 non-null   int64 \n",
            " 8   Face                        5220 non-null   int64 \n",
            " 9   Security: personal          5220 non-null   int64 \n",
            " 10  Security: societal          5220 non-null   int64 \n",
            " 11  Tradition                   5220 non-null   int64 \n",
            " 12  Conformity: rules           5220 non-null   int64 \n",
            " 13  Conformity: interpersonal   5220 non-null   int64 \n",
            " 14  Humility                    5220 non-null   int64 \n",
            " 15  Benevolence: caring         5220 non-null   int64 \n",
            " 16  Benevolence: dependability  5220 non-null   int64 \n",
            " 17  Universalism: concern       5220 non-null   int64 \n",
            " 18  Universalism: nature        5220 non-null   int64 \n",
            " 19  Universalism: tolerance     5220 non-null   int64 \n",
            " 20  Universalism: objectivity   5220 non-null   int64 \n",
            "dtypes: int64(20), object(1)\n",
            "memory usage: 856.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep Data"
      ],
      "metadata": {
        "id": "8Uvfel3EE17z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert multiple label columns to one label list column\n",
        "train_labs_df['labels'] = train_labs_df.loc[:, 'Self-direction: thought':'Universalism: objectivity'].values.tolist()"
      ],
      "metadata": {
        "id": "G6Ov0b5mmKtn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label distribution for full training data\n",
        "print('Self-direction: thought =', sum(train_labs_df['Self-direction: thought']))\n",
        "print('Self-direction: action =', sum(train_labs_df['Self-direction: action']))\n",
        "print('Stimulation =', sum(train_labs_df['Stimulation']))\n",
        "print('Hedonism =', sum(train_labs_df['Hedonism']))\n",
        "print('Achievement = ', sum(train_labs_df['Achievement']))\n",
        "print('Power: dominance =', sum(train_labs_df['Power: dominance']))\n",
        "print('Power: resources =', sum(train_labs_df['Power: resources']))\n",
        "print('Face =', sum(train_labs_df['Face']))\n",
        "print('Security: personal =', sum(train_labs_df['Security: personal']))\n",
        "print('Security: societal =', sum(train_labs_df['Security: societal']))\n",
        "print('Tradition =', sum(train_labs_df['Tradition']))\n",
        "print('Conformity: rules =', sum(train_labs_df['Conformity: rules']))\n",
        "print('Conformity: interpersonal =', sum(train_labs_df['Conformity: interpersonal']))\n",
        "print('Humility =', sum(train_labs_df['Humility']))\n",
        "print('Benevolence: caring =', sum(train_labs_df['Benevolence: caring']))\n",
        "print('Benevolence: dependability =', sum(train_labs_df['Benevolence: dependability']))\n",
        "print('Universalism: concern =', sum(train_labs_df['Universalism: concern']))\n",
        "print('Universalism: nature =', sum(train_labs_df['Universalism: nature']))\n",
        "print('Universalism: tolerance =', sum(train_labs_df['Universalism: tolerance']))\n",
        "print('Universalism: objectivity =', sum(train_labs_df['Universalism: objectivity']))\n",
        "\n",
        "print('\\nTotal number of samples = ', len(train_labs_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frLZrgKw26yv",
        "outputId": "e234db55-4939-4daf-af4e-86150d84bd42"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-direction: thought = 913\n",
            "Self-direction: action = 1332\n",
            "Stimulation = 312\n",
            "Hedonism = 202\n",
            "Achievement =  1400\n",
            "Power: dominance = 461\n",
            "Power: resources = 566\n",
            "Face = 374\n",
            "Security: personal = 1961\n",
            "Security: societal = 1627\n",
            "Tradition = 598\n",
            "Conformity: rules = 1222\n",
            "Conformity: interpersonal = 217\n",
            "Humility = 438\n",
            "Benevolence: caring = 1500\n",
            "Benevolence: dependability = 766\n",
            "Universalism: concern = 1992\n",
            "Universalism: nature = 358\n",
            "Universalism: tolerance = 709\n",
            "Universalism: objectivity = 937\n",
            "\n",
            "Total number of samples =  5220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine dfs to add label list to data dictionary\n",
        "train_merged_df = pd.merge(train_args_df, train_labs_df, on='Argument ID')\n",
        "train_merged_df = train_merged_df.drop(columns=['Self-direction: thought',\n",
        "                                                'Self-direction: action',\n",
        "                                                'Stimulation',\n",
        "                                                'Hedonism',\n",
        "                                                'Achievement',\n",
        "                                                'Power: dominance',\n",
        "                                                'Power: resources',\n",
        "                                                'Face',\n",
        "                                                'Security: personal',\n",
        "                                                'Security: societal',\n",
        "                                                'Tradition',\n",
        "                                                'Conformity: rules',\n",
        "                                                'Conformity: interpersonal',\n",
        "                                                'Humility',\n",
        "                                                'Benevolence: caring',\n",
        "                                                'Benevolence: dependability',\n",
        "                                                'Universalism: concern',\n",
        "                                                'Universalism: nature',\n",
        "                                                'Universalism: tolerance',\n",
        "                                                'Universalism: objectivity'])"
      ],
      "metadata": {
        "id": "349ZmqVtANb_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view structure\n",
        "train_merged_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4Gi6B4Tp-1S",
        "outputId": "9be29bd6-164f-4368-b23f-c40e3cb9f6da"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5220 entries, 0 to 5219\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            " 4   labels       5220 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 244.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split"
      ],
      "metadata": {
        "id": "qOnSRjeqzvf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train data into 80/20 train/val\n",
        "train_data, val_data = train_test_split(train_merged_df, test_size=0.2, random_state=4)"
      ],
      "metadata": {
        "id": "Y4wusIBmzuzD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert each row to a dictionary -> List[Dict]\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "val_data = val_data.to_dict(orient='records')\n",
        "# print examples\n",
        "print('training example:\\n', train_data[0])\n",
        "print('validation example:\\n', val_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvUcmQ5Ijops",
        "outputId": "d935ec56-936d-4fe1-de93-d4202c2331bb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training example:\n",
            " {'Argument ID': 'A07017', 'Conclusion': 'Homeopathy brings more harm than good', 'Stance': 'against', 'Premise': 'homeopathy uses natural remedies that have little to no side affects on the body.', 'labels': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]}\n",
            "validation example:\n",
            " {'Argument ID': 'A18174', 'Conclusion': 'The vow of celibacy should be abandoned', 'Stance': 'against', 'Premise': \"the vow of celibacy should be promoted as it brings the sense of self control and purity to a person's soul.\", 'labels': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "eIFcUB2EcI_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load samples from HuggingFace dataset to be batched and encoded\n",
        "\n",
        "class BatchTokenizer:\n",
        "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
        "    \"\"\"HuggingFace docs: https://huggingface.co/transformers/v3.0.2/preprocessing.html\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer\n",
        "\n",
        "        Args:\n",
        "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
        "        \"\"\"\n",
        "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "    \n",
        "    # HuggingFace tokenizer will join data with sentence separator token\n",
        "    # and match batches of tokenized and encoded sentences\n",
        "    def get_sep_token(self,):\n",
        "        return self.hf_tokenizer.sep_token\n",
        "\n",
        "    # call method can only take a pair of inputs, but we have three\n",
        "    # conclusion batch, stance batch, and premise batch\n",
        "    # so we create a hack\n",
        "    #def __call__(self, con_batch: List[str], stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:\n",
        "\n",
        "    def __call__(self, con_stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:  \n",
        "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
        "\n",
        "        We return a dictionary of tensors per the huggingface model specification.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): A List of sentence strings\n",
        "\n",
        "        Returns:\n",
        "            Dict: The dictionary of token specifications provided by HuggingFace\n",
        "        \"\"\"\n",
        "        # The HF tokenizer will PAD for us, and additionally combine \n",
        "        # the two sentences deimited by the [SEP] token.\n",
        "        enc = self.hf_tokenizer(\n",
        "            con_stan_batch,\n",
        "            prem_batch,\n",
        "            #stan_batch,\n",
        "            #prem_batch,\n",
        "            padding=True,\n",
        "            return_token_type_ids=False, # ignore with hack\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return enc"
      ],
      "metadata": {
        "id": "1qO3HAWVcOwa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = BatchTokenizer()"
      ],
      "metadata": {
        "id": "9suBPQW7tNRu"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer\n",
        "#token_ex = tokenizer(*[['this is the conclusion.', 'this is also a conclusion'], ['this is the stance.', 'this is also a stance'], ['this is the premise', 'this is also a premise']])\n",
        "#tokenizer.hf_tokenizer.batch_decode(token_ex['input_ids'])\n",
        "# HF tokenizer not working for three input sentence types"
      ],
      "metadata": {
        "id": "La66aNazszlf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer without triplet hack (only two input types acceptable)\n",
        "token_ex = tokenizer(*[['this is the conclusion with more words', 'this is also a conclusion'], ['this is the premise', 'this is the second premise']])\n",
        "print(token_ex)\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwTE69y4mPZ4",
        "outputId": "d8c91978-82be-4e6e-dec0-605c2d226107"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  2023,\n",
            "          2003,  1996, 18458,   102],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2023,  2003,  1996,\n",
            "          2117, 18458,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] this is the premise [SEP]',\n",
              " '[CLS] this is also a conclusion [SEP] this is the second premise [SEP] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer with triplet hack\n",
        "token_ex2 = tokenizer(*[['this is the conclusion with more words [SEP] and a stance against', 'this is also a conclusion [SEP] with another stance that is in favor of'], ['this is the premise', 'this is the second premise']])\n",
        "print(token_ex2)\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex2['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSlzDkBzwtl3",
        "outputId": "ef6e3ce9-d62c-4707-fc1d-96037ed5f835"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  1998,\n",
            "          1037, 11032,  2114,   102,  2023,  2003,  1996, 18458,   102,     0,\n",
            "             0,     0],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2007,  2178, 11032,\n",
            "          2008,  2003,  1999,  5684,  1997,   102,  2023,  2003,  1996,  2117,\n",
            "         18458,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] and a stance against [SEP] this is the premise [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] this is also a conclusion [SEP] with another stance that is in favor of [SEP] this is the second premise [SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "e_C6dXRcs4zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate triple-wise inputs\n",
        "\n",
        "def generate_triplewise_input(dataset: List[Dict]) -> (List[str], List[str], List[str], List[List[int]]):\n",
        "    \"\"\"\n",
        "    TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
        "    a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
        "    \"\"\"\n",
        "\n",
        "    # extract each observation from dictionary; save to list\n",
        "    d_vals = []\n",
        "    for i in range(len(dataset)):\n",
        "        d_vals.append(list(dataset[i].values()))\n",
        "\n",
        "    # store data items in lists by three categories    \n",
        "    conclusion_lst = []\n",
        "    stance_lst = []\n",
        "    premise_lst = []\n",
        "\n",
        "    # store labels in list of lists of 20 labels\n",
        "    label_lst = []\n",
        "\n",
        "    # generate separate lists from each observation\n",
        "    for i in range(len(d_vals)):\n",
        "        conclusion_lst.append(d_vals[i][1])\n",
        "        stance_lst.append(d_vals[i][2])\n",
        "        premise_lst.append(d_vals[i][3])\n",
        "        label_lst.append(d_vals[i][4])\n",
        "\n",
        "    # add [SEP] token before every stance in list\n",
        "    stance_lst = [' [SEP] ' + s for s in stance_lst]\n",
        "\n",
        "    return (conclusion_lst, stance_lst, premise_lst, label_lst)"
      ],
      "metadata": {
        "id": "uoLv-VD2s7VW"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to generate triple-wise inputs and labels for batching\n",
        "\n",
        "# training data\n",
        "train_conclusions, train_stances, train_premises, train_labels = generate_triplewise_input(train_data)\n",
        "\n",
        "# validation data\n",
        "val_conclusions, val_stances, val_premises, val_labels = generate_triplewise_input(val_data)"
      ],
      "metadata": {
        "id": "SiVoKuDw5nXd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporarily combine conclusions and stances separate with [SEP]\n",
        "# use hack to merge tokenized conclusion batch, stance batch, and premise batch\n",
        "\n",
        "# training data\n",
        "train_conclusions_stances = []\n",
        "for i in range(len(train_conclusions)):\n",
        "  train_conclusions_stances.append(train_conclusions[i] + train_stances[i])\n",
        "\n",
        "# validation data\n",
        "val_conclusions_stances = []\n",
        "for i in range(len(val_conclusions)):\n",
        "  val_conclusions_stances.append(val_conclusions[i] + val_stances[i])"
      ],
      "metadata": {
        "id": "A-J8F0f84SOF"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define batch size\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "SAHqMaOF3QiO"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to chunk data for batches\n",
        "\n",
        "# for train labels\n",
        "def chunk(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i: i+n]\n",
        "\n",
        "# for train features\n",
        "def chunk_multi(lst1, lst2, n):\n",
        "    for i in range(0, len(lst1), n):\n",
        "        yield lst1[i: i+n], lst2[i: i+n]"
      ],
      "metadata": {
        "id": "HPVATQvA3QfX"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch input data \n",
        "# tokenize and encode simultaneously since we are using HuggingFace\n",
        "\n",
        "# training data\n",
        "# batch\n",
        "train_input_batches = [b for b in chunk_multi(train_conclusions_stances, train_premises, batch_size)]\n",
        "# tokenize + encode\n",
        "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]\n",
        "\n",
        "# validation data\n",
        "# batch\n",
        "val_input_batches = [b for b in chunk_multi(val_conclusions_stances, val_premises, batch_size)]\n",
        "# tokenize + encode\n",
        "val_input_batches = [tokenizer(*batch) for batch in val_input_batches]\n"
      ],
      "metadata": {
        "id": "qW7pLX6r3XN2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check training data example\n",
        "print(train_input_batches[0])\n",
        "encoded_tst = tokenizer.hf_tokenizer.batch_decode(train_input_batches[0]['input_ids'])\n",
        "encoded_tst[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "ueZWWuXw8REg",
        "outputId": "30bcda07-76ab-47c3-ca91-0daf4b3465fd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2188, 29477,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] homeopathy brings more harm than good [SEP] against [SEP] homeopathy uses natural remedies that have little to no side affects on the body. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check training data example\n",
        "print(val_input_batches[0])\n",
        "encoded_tst2 = tokenizer.hf_tokenizer.batch_decode(val_input_batches[0]['input_ids'])\n",
        "encoded_tst2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "cob9vDK-6n3N",
        "outputId": "e259c95c-afca-444f-e836-4503a58daab5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1996, 19076,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2591,  2865,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] the vow of celibacy should be abandoned [SEP] against [SEP] the vow of celibacy should be promoted as it brings the sense of self control and purity to a person's soul. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to batch class labels\n",
        "# a single observation's label is a list of 20 labels\n",
        "\n",
        "def encode_labels(labels: List[List[int]]) -> torch.FloatTensor:\n",
        "    \"\"\"Turns the batch of labels into a tensor\n",
        "\n",
        "    Args:\n",
        "        labels (List[List[int]]): List of all lists of labels in batch\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor: Tensor of all lists of labels in batch\n",
        "    \"\"\"\n",
        "    \n",
        "    return torch.LongTensor(labels)\n"
      ],
      "metadata": {
        "id": "hmjoSi9t3bDK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch labels in same order as inputs\n",
        "# batch\n",
        "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
        "val_label_batches = [b for b in chunk(val_labels, batch_size)]\n",
        "# tokenize + encode\n",
        "train_label_batches = [encode_labels(batch) for batch in train_label_batches]\n",
        "val_label_batches = [encode_labels(batch) for batch in val_label_batches]"
      ],
      "metadata": {
        "id": "qJKaO7Wm3fIp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMDqYTjL9HES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}