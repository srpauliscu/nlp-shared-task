{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srpauliscu/nlp-shared-task/blob/main/semeval_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tasks\n",
        "\n",
        "\n",
        "## data (Caroline)\n",
        "- data augmentation\n",
        "\n",
        "## model (Spencer)\n",
        "\n",
        "\n",
        "## writing (both)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uh7oHL4dmSGF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmwKgojDmFe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script Setup"
      ],
      "metadata": {
        "id": "02Eyvjuw9cbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrbPbw-W_xsp",
        "outputId": "b326a0c4-2a6f-4fbd-f2fc-2328642df4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -r drive/MyDrive/nlp_sp/env/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jnTL9B6_i_K",
        "outputId": "a1754188-2b32-4fdf-841d-33b00962664c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 2)) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 3)) (1.12.1+cu113)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2022.11.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (4.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.24.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import block\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy import logical_and, sum as t_sum"
      ],
      "metadata": {
        "id": "j4MvReNO9e8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup for CUDA\n",
        "\n",
        "'''\n",
        "\n",
        "Important: Every tensor, layer, and model needs to be sent to the same device using to()\n",
        "Ex: \n",
        "  ten = torch.ones(4,5).to(device)\n",
        "\n",
        "'''\n",
        "\n",
        "# TODO: Uncomment when ready to run on GPUs\n",
        "#device = 'cuda' if torch.cuda.isa_available() else 'cpu'\n",
        "device = 'cpu'\n"
      ],
      "metadata": {
        "id": "NErpy8X6-NUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup (TODO)"
      ],
      "metadata": {
        "id": "RuNSOLZz1VXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Below is the code to define our model as well as the training loop."
      ],
      "metadata": {
        "id": "wp873QUQ8nEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "VA8Wt2R_8xc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLIClassifier(torch.nn.Module):\n",
        "    def __init__(self, output_size: int, hidden_size: int, dropout_prob: float):\n",
        "      \n",
        "      # Basic initialization\n",
        "      super().__init__()\n",
        "      self.output_size = output_size\n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "      # Additional args\n",
        "      self.dropout_prob = dropout_prob\n",
        "\n",
        "      # Initialize BERT, which we use instead of a single embedding layer.\n",
        "      self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
        "      \n",
        "      # Comment out these lines to unfreeze BERT params\n",
        "      for param in self.bert.parameters():\n",
        "          param.requires_grad = False\n",
        "          \n",
        "      # Get BERT's hiddem dim\n",
        "      self.bert_hidden_dimension = self.bert.config.hidden_size\n",
        "      \n",
        "      \n",
        "      # Single linear layer to project to hidden size\n",
        "      self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
        "      \n",
        "      # Use RELU regularization\n",
        "      # TODO: Could try others\n",
        "      self.relu = torch.nn.ReLU()\n",
        "\n",
        "      '''\n",
        "\n",
        "      We are doing multi-label classification using a chain classifier.\n",
        "      For details, see: https://en.wikipedia.org/wiki/Multi-label_classification\n",
        "\n",
        "      Setup a classifier chain for the 20 labels.\n",
        "      To simplify code, just store them in a list and run through them sequentially.\n",
        "      They will be interpreted in the same order as the training data:\n",
        "\n",
        "      Self-direction: thought\n",
        "      Self-direction: action\n",
        "      Stimulation\n",
        "      Hedonism\n",
        "      Achievement\n",
        "      Power: dominance\n",
        "      Power: resources\n",
        "      Face\n",
        "      Security: personal\n",
        "      Security: societal\n",
        "      Tradition\n",
        "      Conformity: rules\n",
        "      Conformity: interpersonal\n",
        "      Humility\n",
        "      Benevolence: caring\n",
        "      Benevolence: dependability\n",
        "      Universalism: concern\n",
        "      Universalism: nature\n",
        "      Universalism: tolerance\n",
        "      Universalism: objectivity\n",
        "\n",
        "      '''\n",
        "\n",
        "      self.chain = []\n",
        "      for i in range(self.output_size):\n",
        "\n",
        "        # To make it a chain, the output of the previous classifier is \n",
        "        # appended to the input and used as the input for the next classifier\n",
        "\n",
        "        # TODO: do we softmax here?\n",
        "        # TODO: should we initialize each layer with random weights?\n",
        "        t = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(in_features=self.hidden_size + i, out_features = 1),\n",
        "            torch.nn.LogSoftmax(dim=2)\n",
        "        )\n",
        "        self.chain.append(t.to(device))\n",
        "      \n",
        "      # Simple one-layer classifier with a softmax activation function\n",
        "      #self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "      #self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def encode_text(\n",
        "        self,\n",
        "        symbols: Dict\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Use BERT to create contextulized embeddings and get the output \n",
        "            from the pooling layer (i.e. embedding for CLR)\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoding of CLR for the given input\n",
        "        \"\"\"\n",
        "\n",
        "        # Run through BERT for contextualized embeddings\n",
        "        encoded_sequence = self.bert(**symbols)\n",
        "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
        "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "        #      and check the returns for the forward method.\n",
        "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
        "        \n",
        "        # Pooler output is initially (batch_size, bert_hidden_dimension)\n",
        "        pool_out = torch.unsqueeze(encoded_sequence['pooler_output'], dim=1)\n",
        "        return pool_out\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        symbols: Dict,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: _description_\n",
        "        \"\"\"\n",
        "        encoded_sents = self.encode_text(symbols)\n",
        "        output = self.hidden_layer(encoded_sents)\n",
        "        output = self.relu(output)\n",
        "        \n",
        "        # output is of size (batch_size, hidden_layer)\n",
        "\n",
        "        # Run through the classifier chain\n",
        "        cur_input = output\n",
        "        logits = []\n",
        "        labels = []\n",
        "        for classifier in self.chain:\n",
        "\n",
        "          # Get output of next in chain\n",
        "          o = classifier(cur_input)\n",
        "\n",
        "          # TODO: Save the logits for training?\n",
        "          logits.append(o)\n",
        "\n",
        "          # Make a prediction\n",
        "          # TODO: is this correct?\n",
        "          pred = torch.argmax(o, axis=2).squeeze()\n",
        "\n",
        "          # Add that pred to the list\n",
        "          labels.append(pred)\n",
        "\n",
        "          # Append each previous prediction to the input for the next classifier\n",
        "          for p in labels:\n",
        "\n",
        "            # TODO: what dim do I use here?\n",
        "            cur_input = torch.cat((cur_input, p), dim=0)\n",
        "\n",
        "        #output = self.classifier(output)\n",
        "        #return self.log_softmax(output)\n",
        "\n",
        "        # Return a list of logits from all of our classifiers\n",
        "        return logits"
      ],
      "metadata": {
        "id": "4XXLNXdp8oOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "7A0BDNUl0TZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Make Predictions for Evaluation"
      ],
      "metadata": {
        "id": "0COIGOg6z_J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
        "    logits = model(sents)\n",
        "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
      ],
      "metadata": {
        "id": "uAzIT5nq0EKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Functions"
      ],
      "metadata": {
        "id": "JBDUhwxH0Xpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Precision is True Positives / All Positives Predictions\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(pred_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "        \n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def recall(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Recall is True Positives / All Positive Labels\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(true_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def f1_score(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    which_label: int\n",
        "):\n",
        "    \"\"\"\n",
        "    F1 score is the harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
        "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
        "    if P and R:\n",
        "        return 2*P*R/(P+R)\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def macro_f1(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    possible_labels: List[int]\n",
        "):\n",
        "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
        "    # Macro, so we take the uniform avg.\n",
        "    return sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "aHcGkLzv0dsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "eCKKh-tLzWV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(\n",
        "    num_epochs,\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    dev_features,\n",
        "    dev_labels,\n",
        "    optimizer,\n",
        "    model,\n",
        "    possible_labels\n",
        "):\n",
        "    print(\"Training...\")\n",
        "    dev_f1_scores = []\n",
        "    loss_func = torch.nn.NLLLoss()\n",
        "    batches = list(zip(train_features, train_labels))\n",
        "    random.shuffle(batches)\n",
        "    for i in range(num_epochs):\n",
        "        losses = []\n",
        "        for features, labels in tqdm(batches):\n",
        "            # Empty the dynamic computation graph\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(features).squeeze(1)\n",
        "            loss = loss_func(preds, labels)\n",
        "            # Backpropogate the loss through our model\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
        "        # Estimate the f1 score for the development set\n",
        "        print(\"Evaluating dev...\")\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for sents, labels in tqdm(zip(dev_features, dev_labels), total=len(dev_features)):\n",
        "            pred = predict(model, sents)\n",
        "            all_preds.extend(pred)\n",
        "            all_labels.extend(list(labels.numpy()))\n",
        "\n",
        "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
        "        print(f\"Dev F1 {dev_f1}\")\n",
        "        dev_f1_scores.append(dev_f1)\n",
        "        #print(all_preds)\n",
        "        \n",
        "    # Print the best dev_f1 score for result reporting\n",
        "    print(f\"Best dev F1 score: {np.max(dev_f1_scores)}\")\n",
        "    # Return the trained model\n",
        "    return model"
      ],
      "metadata": {
        "id": "Xp81C1FczX_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Phase"
      ],
      "metadata": {
        "id": "J56VAQ-k1uF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "ewBJG2kK108X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training loops\n",
        "epochs = 20\n",
        "\n",
        "# Learning rate - should be very small when using Adam\n",
        "LR = .0001\n",
        "\n",
        "# Dropout probability\n",
        "dropout_prob = 0.2"
      ],
      "metadata": {
        "id": "uW1dGprF140O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "82BHsUDb2KYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of labels (should be 20)\n",
        "possible_labels = len(set(train_labels))\n",
        "if possible_labels != 20:\n",
        "  raise RuntimeError(f\"Instead of 20 possible labels, we found {possible_labels}.\")\n",
        "\n",
        "# Intialize model\n",
        "model = NLIClassifier(output_size=possible_labels, hidden_size = 512, dropout_prob=dropout_prob)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
        "\n",
        "# Setup the validation set\n",
        "validation_input_batches = [b for b in chunk_multi(validation_premises, validation_hypotheses, batch_size)]\n",
        "# Tokenize + encode\n",
        "validation_input_batches = [tokenizer(*batch) for batch in validation_input_batches]\n",
        "validation_batch_labels = [b for b in chunk(validation_labels, batch_size)]\n",
        "validation_batch_labels = [encode_labels(batch) for batch in validation_batch_labels]\n"
      ],
      "metadata": {
        "id": "6Uvma7511u01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "W9vCMzYO3oVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training\n",
        "trained_model = training_loop(\n",
        "    epochs,\n",
        "    train_input_batches,\n",
        "    train_label_batches,\n",
        "    validation_input_batches,\n",
        "    validation_batch_labels,\n",
        "    optimizer,\n",
        "    model,\n",
        "    list(range(possible_labels))\n",
        ")\n"
      ],
      "metadata": {
        "id": "hhmsgNFE3p1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}