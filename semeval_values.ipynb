{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JBDUhwxH0Xpm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srpauliscu/nlp-shared-task/blob/main/semeval_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script Setup"
      ],
      "metadata": {
        "id": "02Eyvjuw9cbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LrbPbw-W_xsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -r drive/MyDrive/nlp_sp/env/requirements.txt"
      ],
      "metadata": {
        "id": "1jnTL9B6_i_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import block\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy import logical_and, sum as t_sum\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "j4MvReNO9e8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup for CUDA\n",
        "\n",
        "'''\n",
        "\n",
        "Important: Every tensor, layer, and model needs to be sent to the same device using to()\n",
        "Ex: \n",
        "  ten = torch.ones(4,5).to(device)\n",
        "\n",
        "'''\n",
        "\n",
        "# Get the best device to run on\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ],
      "metadata": {
        "id": "NErpy8X6-NUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b8727d-8861-47f8-e15d-d4bf6fb8da4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "JLflCMhp4IVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define hyparameters near top to make changing them easier"
      ],
      "metadata": {
        "id": "cGpJThBt4QHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training loops\n",
        "epochs = 100\n",
        "\n",
        "# Learning rate - should be very small when using Adam\n",
        "LR = .001\n",
        "\n",
        "# Dropout probability\n",
        "dropout_prob = 0.0\n",
        "\n",
        "# Batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Size to project to after BERT\n",
        "hidden_size = 1024"
      ],
      "metadata": {
        "id": "jfFOC_WZ4Hm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "RuNSOLZz1VXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Format\n",
        "\n",
        "**Data:** `arguments-training/validation/testing.tsv`\n",
        "(5220 arguments)\n",
        "- Argument ID\n",
        "- Conclusion \n",
        "- Stance (e.g., in favor, against)\n",
        "- Premise (justification for conclusion)\n",
        "\n",
        "**Labels:** `labels-training/validation/testing.tsv` \n",
        "(20 binary value labels per argument)\n",
        "- Argument ID\n",
        "- Self-direction: thought\n",
        "- Self-direction: action\n",
        "- Stimulation\n",
        "- Hedonism\n",
        "- Achievement\n",
        "- Power: dominance\n",
        "- Power: resources\n",
        "- Face\n",
        "- Security: personal\n",
        "- Security: societal\n",
        "- Tradition\n",
        "- Conformity: rules\n",
        "- Conformity: interpersonal\n",
        "- Humility\n",
        "- Benevolence: caring\n",
        "- Benevolence: dependability\n",
        "- Universalism: concern\n",
        "- Universalism: nature\n",
        "- Universalism: tolerance\n",
        "- Universalism: objectivity\n",
        "\n",
        "**Access:** https://doi.org/10.5281/zenodo.6814563"
      ],
      "metadata": {
        "id": "I3ZSPKuiyXxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "jwPoXtFRzPub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training arguments\n",
        "#train_args_df = pd.read_csv('/content/drive/MyDrive/nlp_sp/data/arguments-training.tsv', sep='\\t')         # Spencer\n",
        "train_args_df = pd.read_csv('/content/drive/MyDrive/csci5832_project/data/arguments-training.tsv', sep='\\t') # Caroline\n",
        "# view structure\n",
        "train_args_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP428yMAz6GZ",
        "outputId": "bb05878f-cf7b-4de4-924c-e3f08f487e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 163.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training labels\n",
        "#train_labs_df = pd.read_csv('/content/drive/MyDrive/nlp_sp/data/labels-training.tsv', sep='\\t')         # Spencer\n",
        "train_labs_df = pd.read_csv('/content/drive/MyDrive/csci5832_project/data/labels-training.tsv', sep='\\t') # Caroline\n",
        "# view structure\n",
        "train_labs_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKlU8wD61Jfq",
        "outputId": "6e35c2d6-25b0-4206-efbf-840ccd710ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 21 columns):\n",
            " #   Column                      Non-Null Count  Dtype \n",
            "---  ------                      --------------  ----- \n",
            " 0   Argument ID                 5220 non-null   object\n",
            " 1   Self-direction: thought     5220 non-null   int64 \n",
            " 2   Self-direction: action      5220 non-null   int64 \n",
            " 3   Stimulation                 5220 non-null   int64 \n",
            " 4   Hedonism                    5220 non-null   int64 \n",
            " 5   Achievement                 5220 non-null   int64 \n",
            " 6   Power: dominance            5220 non-null   int64 \n",
            " 7   Power: resources            5220 non-null   int64 \n",
            " 8   Face                        5220 non-null   int64 \n",
            " 9   Security: personal          5220 non-null   int64 \n",
            " 10  Security: societal          5220 non-null   int64 \n",
            " 11  Tradition                   5220 non-null   int64 \n",
            " 12  Conformity: rules           5220 non-null   int64 \n",
            " 13  Conformity: interpersonal   5220 non-null   int64 \n",
            " 14  Humility                    5220 non-null   int64 \n",
            " 15  Benevolence: caring         5220 non-null   int64 \n",
            " 16  Benevolence: dependability  5220 non-null   int64 \n",
            " 17  Universalism: concern       5220 non-null   int64 \n",
            " 18  Universalism: nature        5220 non-null   int64 \n",
            " 19  Universalism: tolerance     5220 non-null   int64 \n",
            " 20  Universalism: objectivity   5220 non-null   int64 \n",
            "dtypes: int64(20), object(1)\n",
            "memory usage: 856.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "whsRrGx71cUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert multiple label columns to one label list column\n",
        "train_labs_df['labels'] = train_labs_df.loc[:, 'Self-direction: thought':'Universalism: objectivity'].values.tolist()"
      ],
      "metadata": {
        "id": "33okLWjn1poE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label distribution for full training data\n",
        "print('Self-direction: thought =', sum(train_labs_df['Self-direction: thought']))\n",
        "print('Self-direction: action =', sum(train_labs_df['Self-direction: action']))\n",
        "print('Stimulation =', sum(train_labs_df['Stimulation']))\n",
        "print('Hedonism =', sum(train_labs_df['Hedonism']))\n",
        "print('Achievement = ', sum(train_labs_df['Achievement']))\n",
        "print('Power: dominance =', sum(train_labs_df['Power: dominance']))\n",
        "print('Power: resources =', sum(train_labs_df['Power: resources']))\n",
        "print('Face =', sum(train_labs_df['Face']))\n",
        "print('Security: personal =', sum(train_labs_df['Security: personal']))\n",
        "print('Security: societal =', sum(train_labs_df['Security: societal']))\n",
        "print('Tradition =', sum(train_labs_df['Tradition']))\n",
        "print('Conformity: rules =', sum(train_labs_df['Conformity: rules']))\n",
        "print('Conformity: interpersonal =', sum(train_labs_df['Conformity: interpersonal']))\n",
        "print('Humility =', sum(train_labs_df['Humility']))\n",
        "print('Benevolence: caring =', sum(train_labs_df['Benevolence: caring']))\n",
        "print('Benevolence: dependability =', sum(train_labs_df['Benevolence: dependability']))\n",
        "print('Universalism: concern =', sum(train_labs_df['Universalism: concern']))\n",
        "print('Universalism: nature =', sum(train_labs_df['Universalism: nature']))\n",
        "print('Universalism: tolerance =', sum(train_labs_df['Universalism: tolerance']))\n",
        "print('Universalism: objectivity =', sum(train_labs_df['Universalism: objectivity']))\n",
        "\n",
        "print('\\nTotal number of samples = ', len(train_labs_df))"
      ],
      "metadata": {
        "id": "P9jaFRCB3clx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67714712-a000-4623-8782-b746191fd74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-direction: thought = 913\n",
            "Self-direction: action = 1332\n",
            "Stimulation = 312\n",
            "Hedonism = 202\n",
            "Achievement =  1400\n",
            "Power: dominance = 461\n",
            "Power: resources = 566\n",
            "Face = 374\n",
            "Security: personal = 1961\n",
            "Security: societal = 1627\n",
            "Tradition = 598\n",
            "Conformity: rules = 1222\n",
            "Conformity: interpersonal = 217\n",
            "Humility = 438\n",
            "Benevolence: caring = 1500\n",
            "Benevolence: dependability = 766\n",
            "Universalism: concern = 1992\n",
            "Universalism: nature = 358\n",
            "Universalism: tolerance = 709\n",
            "Universalism: objectivity = 937\n",
            "\n",
            "Total number of samples =  5220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine dfs to add label list to data dictionary\n",
        "train_merged_df = pd.merge(train_args_df, train_labs_df, on='Argument ID')\n",
        "train_merged_df = train_merged_df.drop(columns=['Self-direction: thought',\n",
        "                                                'Self-direction: action',\n",
        "                                                'Stimulation',\n",
        "                                                'Hedonism',\n",
        "                                                'Achievement',\n",
        "                                                'Power: dominance',\n",
        "                                                'Power: resources',\n",
        "                                                'Face',\n",
        "                                                'Security: personal',\n",
        "                                                'Security: societal',\n",
        "                                                'Tradition',\n",
        "                                                'Conformity: rules',\n",
        "                                                'Conformity: interpersonal',\n",
        "                                                'Humility',\n",
        "                                                'Benevolence: caring',\n",
        "                                                'Benevolence: dependability',\n",
        "                                                'Universalism: concern',\n",
        "                                                'Universalism: nature',\n",
        "                                                'Universalism: tolerance',\n",
        "                                                'Universalism: objectivity'])"
      ],
      "metadata": {
        "id": "tH2O2t-d1r4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view structure\n",
        "train_merged_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ACBME6l1xph",
        "outputId": "6bf972cf-5324-45c0-8f45-ed0099c2e2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5220 entries, 0 to 5219\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            " 4   labels       5220 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 244.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val Split"
      ],
      "metadata": {
        "id": "ylBInj2l3f82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train data into 80/20 train/val\n",
        "train_data, val_data = train_test_split(train_merged_df, test_size=0.2, random_state=4)"
      ],
      "metadata": {
        "id": "j40FHbSy1Jo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert each row to a dictionary -> List[Dict]\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "val_data = val_data.to_dict(orient='records')\n",
        "full_data = train_merged_df.to_dict(orient='records')\n",
        "# print examples\n",
        "print('training example:\\n', train_data[0])\n",
        "print('validation example:\\n', val_data[0])\n",
        "print('full example:\\n', full_data[0])"
      ],
      "metadata": {
        "id": "RMELWMrk1JmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73710f7-985c-4a5c-e069-76bd1b4647eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training example:\n",
            " {'Argument ID': 'A07017', 'Conclusion': 'Homeopathy brings more harm than good', 'Stance': 'against', 'Premise': 'homeopathy uses natural remedies that have little to no side affects on the body.', 'labels': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]}\n",
            "validation example:\n",
            " {'Argument ID': 'A18174', 'Conclusion': 'The vow of celibacy should be abandoned', 'Stance': 'against', 'Premise': \"the vow of celibacy should be promoted as it brings the sense of self control and purity to a person's soul.\", 'labels': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0]}\n",
            "full example:\n",
            " {'Argument ID': 'A01001', 'Conclusion': 'Entrapment should be legalized', 'Stance': 'in favor of', 'Premise': \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\", 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the label density\n",
        "numerator = 0\n",
        "for sample in full_data:\n",
        "  numerator += sum(sample['labels'])\n",
        "\n",
        "density = numerator / (20 * len(full_data))\n",
        "\n",
        "# Set the threshold\n",
        "threshold = 2*density\n",
        "#threshold = 0.5\n",
        "\n",
        "print(threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq4CmB8QTcQL",
        "outputId": "d6754d83-fa1c-4f59-ec8a-a5de4a783ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.34262452107279695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "8rX8q7I22cmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load samples from HuggingFace dataset to be batched and encoded\n",
        "\n",
        "class BatchTokenizer:\n",
        "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
        "    \"\"\"HuggingFace docs: https://huggingface.co/transformers/v3.0.2/preprocessing.html\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer\n",
        "\n",
        "        Args:\n",
        "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
        "        \"\"\"\n",
        "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "    \n",
        "    # HuggingFace tokenizer will join data with sentence separator token\n",
        "    # and match batches of tokenized and encoded sentences\n",
        "    def get_sep_token(self,):\n",
        "        return self.hf_tokenizer.sep_token\n",
        "\n",
        "    # call method can only take a pair of inputs, but we have three\n",
        "    # conclusion batch, stance batch, and premise batch\n",
        "    # so we create a hack\n",
        "    #def __call__(self, con_batch: List[str], stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:\n",
        "\n",
        "    def __call__(self, con_stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:  \n",
        "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
        "\n",
        "        We return a dictionary of tensors per the huggingface model specification.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): A List of sentence strings\n",
        "\n",
        "        Returns:\n",
        "            Dict: The dictionary of token specifications provided by HuggingFace\n",
        "        \"\"\"\n",
        "        # The HF tokenizer will PAD for us, and additionally combine \n",
        "        # the two sentences deimited by the [SEP] token.\n",
        "        enc = self.hf_tokenizer(\n",
        "            con_stan_batch,\n",
        "            prem_batch,\n",
        "            #stan_batch,\n",
        "            #prem_batch,\n",
        "            padding=True,\n",
        "            return_token_type_ids=False, # ignore with hack\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return enc"
      ],
      "metadata": {
        "id": "AIy5AQQy2gzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = BatchTokenizer()"
      ],
      "metadata": {
        "id": "lg4r_uFi2oHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer without triplet hack (only two input types acceptable)\n",
        "token_ex = tokenizer(*[['this is the conclusion with more words', 'this is also a conclusion'], ['this is the premise', 'this is the second premise']])\n",
        "print(f\"{token_ex}\\n\")\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAUbG6kb2rLs",
        "outputId": "f911140e-8f61-4702-a1ef-a93e003360da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  2023,\n",
            "          2003,  1996, 18458,   102],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2023,  2003,  1996,\n",
            "          2117, 18458,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] this is the premise [SEP]',\n",
              " '[CLS] this is also a conclusion [SEP] this is the second premise [SEP] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer with triplet hack\n",
        "token_ex2 = tokenizer(*[['this is the conclusion with more words [SEP] and a stance against', 'this is also a conclusion [SEP] with another stance that is in favor of'], ['this is the premise', 'this is the second premise']])\n",
        "print(f\"{token_ex2}\\n\")\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex2['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbJQZe7286L",
        "outputId": "1ebdb4c5-c51f-4d64-8f59-7e0c8c4b9409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  1998,\n",
            "          1037, 11032,  2114,   102,  2023,  2003,  1996, 18458,   102,     0,\n",
            "             0,     0],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2007,  2178, 11032,\n",
            "          2008,  2003,  1999,  5684,  1997,   102,  2023,  2003,  1996,  2117,\n",
            "         18458,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] and a stance against [SEP] this is the premise [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] this is also a conclusion [SEP] with another stance that is in favor of [SEP] this is the second premise [SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "iA1nphB83Yln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate triple-wise inputs\n",
        "\n",
        "def generate_triplewise_input(dataset: List[Dict]) -> (List[str], List[str], List[str], List[str], List[List[int]]):\n",
        "    \"\"\"\n",
        "    group all argument components and corresponding labels of the datapoints\n",
        "    a datapoint is now a dictionary of \n",
        "    argument id, conclusion, stance, premise, and label list\n",
        "    \"\"\"\n",
        "\n",
        "    # extract each observation from dictionary; save to list\n",
        "    d_vals = []\n",
        "    for i in range(len(dataset)):\n",
        "        d_vals.append(list(dataset[i].values()))\n",
        "\n",
        "    # store data items in lists by three categories by id\n",
        "    id_lst = []    \n",
        "    conclusion_lst = []\n",
        "    stance_lst = []\n",
        "    premise_lst = []\n",
        "\n",
        "    # store labels in list of lists of 20 labels\n",
        "    label_lst = []\n",
        "\n",
        "    # generate separate lists from each observation\n",
        "    for i in range(len(d_vals)):\n",
        "        id_lst.append(d_vals[i][0])\n",
        "        conclusion_lst.append(d_vals[i][1])\n",
        "        stance_lst.append(d_vals[i][2])\n",
        "        premise_lst.append(d_vals[i][3])\n",
        "        label_lst.append(d_vals[i][4])\n",
        "\n",
        "    # add [SEP] token before every stance in list\n",
        "    stance_lst = [' [SEP] ' + s for s in stance_lst]\n",
        "\n",
        "    return id_lst, conclusion_lst, stance_lst, premise_lst, label_lst"
      ],
      "metadata": {
        "id": "gYrXwQOR3f7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to generate triple-wise inputs and labels for batching\n",
        "\n",
        "# training data\n",
        "train_ids, train_conclusions, train_stances, train_premises, train_labels = generate_triplewise_input(train_data)\n",
        "\n",
        "# validation data\n",
        "val_ids, val_conclusions, val_stances, val_premises, val_labels = generate_triplewise_input(val_data)\n",
        "\n",
        "# full data\n",
        "full_ids, full_conclusions, full_stances, full_premises, full_labels = generate_triplewise_input(full_data)"
      ],
      "metadata": {
        "id": "SiVoKuDw5nXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporarily combine conclusions and stances separate with [SEP]\n",
        "# use hack to merge tokenized conclusion batch, stance batch, and premise batch\n",
        "\n",
        "# training data\n",
        "train_conclusions_stances = []\n",
        "for i in range(len(train_conclusions)):\n",
        "  train_conclusions_stances.append(train_conclusions[i] + train_stances[i])\n",
        "\n",
        "# validation data\n",
        "val_conclusions_stances = []\n",
        "for i in range(len(val_conclusions)):\n",
        "  val_conclusions_stances.append(val_conclusions[i] + val_stances[i])\n",
        "\n",
        "# full data\n",
        "full_conclusions_stances = []\n",
        "for i in range(len(full_conclusions)):\n",
        "  full_conclusions_stances.append(full_conclusions[i] + full_stances[i])"
      ],
      "metadata": {
        "id": "9htFdXaY33np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to chunk data for batches\n",
        "\n",
        "# for train labels\n",
        "def chunk(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i: i+n]\n",
        "\n",
        "# for train features\n",
        "def chunk_multi(lst1, lst2, n):\n",
        "    for i in range(0, len(lst1), n):\n",
        "        yield lst1[i: i+n], lst2[i: i+n]"
      ],
      "metadata": {
        "id": "tRfM1M-C4vvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch input data \n",
        "# tokenize and encode simultaneously since we are using HuggingFace\n",
        "\n",
        "# batch\n",
        "train_input_batches = [b for b in chunk_multi(train_conclusions_stances, train_premises, batch_size)]\n",
        "val_size = 1\n",
        "full_size = 1\n",
        "val_input_batches = [b for b in chunk_multi(val_conclusions_stances, val_premises, val_size)]\n",
        "full_input_batches = [b for b in chunk_multi(full_conclusions_stances, full_premises, full_size)]\n",
        "\n",
        "# tokenize + encode\n",
        "train_input_batches = [tokenizer(*batch).to(device) for batch in train_input_batches]\n",
        "val_input_batches = [tokenizer(*batch).to(device) for batch in val_input_batches]\n",
        "full_input_batches = [tokenizer(*batch).to(device) for batch in full_input_batches]"
      ],
      "metadata": {
        "id": "Nx_Gt6Iv4ywV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check training data example\n",
        "print(train_input_batches[0])\n",
        "encoded_tst = tokenizer.hf_tokenizer.batch_decode(train_input_batches[0]['input_ids'])\n",
        "encoded_tst[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "g3aH-j365Ync",
        "outputId": "3c53f739-0550-45ae-b665-137b0949ccf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2188, 29477,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2323,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] homeopathy brings more harm than good [SEP] against [SEP] homeopathy uses natural remedies that have little to no side affects on the body. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to batch class labels\n",
        "# a single observation's label is a list of 20 labels\n",
        "\n",
        "def encode_labels(labels: List[List[int]]) -> torch.FloatTensor:\n",
        "    \"\"\"Turns the batch of labels into a tensor\n",
        "\n",
        "    Args:\n",
        "        labels (List[List[int]]): List of all lists of labels in batch\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor: Tensor of all lists of labels in batch\n",
        "    \"\"\"\n",
        "    \n",
        "    return torch.LongTensor(labels)\n"
      ],
      "metadata": {
        "id": "8KWtgfqQ5d3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch labels in same order as inputs\n",
        "# batch\n",
        "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
        "val_label_batches = [b for b in chunk(val_labels, val_size)]\n",
        "full_label_batches = [b for b in chunk(full_labels, full_size)]\n",
        "# tokenize + encode\n",
        "train_label_batches = [encode_labels(batch).to(device) for batch in train_label_batches]\n",
        "val_label_batches = [encode_labels(batch).to(device) for batch in val_label_batches]\n",
        "full_label_batches = [encode_labels(batch).to(device) for batch in full_label_batches]"
      ],
      "metadata": {
        "id": "KysnHWFM5haJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Below is the code to define our model as well as the training loop."
      ],
      "metadata": {
        "id": "wp873QUQ8nEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Make Predictions"
      ],
      "metadata": {
        "id": "yTTeQ4A9jDTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(logits: torch.Tensor) -> torch.Tensor:\n",
        "  # Use boolean logic to handle the predictions\n",
        "  return (logits>threshold).float()\n",
        "\n",
        "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
        "    logits = model(sents)\n",
        "    return make_prediction(logits.cpu())"
      ],
      "metadata": {
        "id": "CotfrOnqjByn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "VA8Wt2R_8xc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to initialize weights for the chain classifiers\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "class NLIClassifier(torch.nn.Module):\n",
        "    def __init__(self, output_size: int, hidden_size: int, dropout_prob: float):\n",
        "      \n",
        "      # Basic initialization\n",
        "      super().__init__()\n",
        "      self.output_size = output_size\n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "      # Additional args\n",
        "      self.dropout_prob = dropout_prob\n",
        "\n",
        "      # Initialize BERT, which we use instead of a single embedding layer.\n",
        "      self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\").to(device)\n",
        "      \n",
        "      # Comment out these lines to unfreeze BERT params\n",
        "      for param in self.bert.parameters():\n",
        "          param.requires_grad = False\n",
        "          \n",
        "      # Get BERT's hiddem dim\n",
        "      self.bert_hidden_dimension = self.bert.config.hidden_size\n",
        "      \n",
        "      \n",
        "      # Single linear layer to project to hidden size\n",
        "      self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size * 4).to(device)\n",
        "      self.hidden_layer_2 = torch.nn.Linear(self.hidden_size * 4, self.hidden_size * 2).to(device)\n",
        "      #self.hidden_layer_3 = torch.nn.Linear(self.hidden_size * 2, self.hidden_size).to(device)\n",
        "      \n",
        "      # Use RELU regularization\n",
        "      # TODO: Could try others\n",
        "      self.relu = torch.nn.ReLU()\n",
        "\n",
        "      '''\n",
        "\n",
        "      We are doing multi-label classification using a chain classifier.\n",
        "      For details, see: https://en.wikipedia.org/wiki/Multi-label_classification\n",
        "\n",
        "      Setup a classifier chain for the 20 labels.\n",
        "      To simplify code, just store them in a list and run through them sequentially.\n",
        "      They will be interpreted in the same order as the training data:\n",
        "\n",
        "      Self-direction: thought\n",
        "      Self-direction: action\n",
        "      Stimulation\n",
        "      Hedonism\n",
        "      Achievement\n",
        "      Power: dominance\n",
        "      Power: resources\n",
        "      Face\n",
        "      Security: personal\n",
        "      Security: societal\n",
        "      Tradition\n",
        "      Conformity: rules\n",
        "      Conformity: interpersonal\n",
        "      Humility\n",
        "      Benevolence: caring\n",
        "      Benevolence: dependability\n",
        "      Universalism: concern\n",
        "      Universalism: nature\n",
        "      Universalism: tolerance\n",
        "      Universalism: objectivity\n",
        "\n",
        "      '''\n",
        "\n",
        "      self.chain = []\n",
        "      for i in range(self.output_size):\n",
        "\n",
        "        # To make it a chain, the prediction from the previous classifier is \n",
        "        # appended to the input and used as the input for the next classifier\n",
        "\n",
        "        # Initialize each chain classifier\n",
        "        # TODO: Try more layers per classifier\n",
        "        # TODO: Could try bigger BERT model, but that would require more changes\n",
        "        # TODO: Could unfreeze BERT weights\n",
        "        \n",
        "        # TODO: Hyperparameter tunings\n",
        "\n",
        "        # TODO: Could also play with the threshold for prediction\n",
        "        # and base it on label cardinality (i.e. average number of labels per sample)\n",
        "\n",
        "        t = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(in_features=self.hidden_size + i, out_features = 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.chain.append(t.to(device))\n",
        "        # Initialize the weights\n",
        "        for c in self.chain:\n",
        "          c.apply(init_weights)\n",
        "\n",
        "    def encode_text(\n",
        "        self,\n",
        "        symbols: Dict\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Use BERT to create contextulized embeddings and get the output \n",
        "            from the pooling layer (i.e. embedding for CLR)\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoding of CLR for the given input\n",
        "        \"\"\"\n",
        "\n",
        "        # Run through BERT for contextualized embeddings\n",
        "        encoded_sequence = self.bert(**symbols)\n",
        "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
        "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "        #      and check the returns for the forward method.\n",
        "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
        "        \n",
        "        # Pooler output is initially (batch_size, bert_hidden_dimension)\n",
        "        pool_out = torch.unsqueeze(encoded_sequence['pooler_output'], dim=1)\n",
        "        return pool_out\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        symbols: Dict,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: _description_\n",
        "        \"\"\"\n",
        "        encoded_sents = self.encode_text(symbols)\n",
        "        output = self.hidden_layer(encoded_sents)\n",
        "        output = self.relu(output)\n",
        "        output = self.hidden_layer_2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.hidden_layer_3(output)\n",
        "        output = self.relu(output)\n",
        "        \n",
        "        # output is of size (batch_size, hidden_layer)\n",
        "\n",
        "        # Run through the classifier chain\n",
        "\n",
        "        cur_input = output\n",
        "        logits = []\n",
        "\n",
        "        for classifier in self.chain:\n",
        "\n",
        "          # Get output of next in chain\n",
        "          o = classifier(cur_input)\n",
        "\n",
        "          # Save the logits for training\n",
        "          logits.append(o)\n",
        "\n",
        "          # Make a prediction so we can append it to the next input\n",
        "          # TODO: Could also append raw logits, potentially\n",
        "          pred = make_prediction(o)\n",
        "\n",
        "          # Append the previous prediction to the input for the next classifier\n",
        "          cur_input = torch.cat([cur_input, pred], dim=2)\n",
        "\n",
        "        # Preds contains 20 tensors, each batch_size x 1 x 1\n",
        "        # We need to return one tensor that is 128 x 20\n",
        "        stack = logits[0].squeeze(dim=1)\n",
        "        for logit in logits[1:]:\n",
        "          stack = torch.cat([stack, logit.squeeze(dim=1)], dim=-1)\n",
        "        \n",
        "        return stack"
      ],
      "metadata": {
        "id": "4XXLNXdp8oOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "7A0BDNUl0TZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Functions"
      ],
      "metadata": {
        "id": "JBDUhwxH0Xpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(predicted_labels, true_labels):\n",
        "    \"\"\"\n",
        "    Precision is True Positives / All Positives Predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Each pred/true pair is a list of 20 values, so need to go one level deeper\n",
        "\n",
        "    all_pos = 0\n",
        "    true_pos = 0\n",
        "    for i in range(len(predicted_labels)):\n",
        "      cur_pred = predicted_labels[i]\n",
        "      cur_true = true_labels[i]\n",
        "\n",
        "      # Count both true_pos and false_pos\n",
        "      all_pos += sum(cur_pred)\n",
        "\n",
        "      # Get true_pos only\n",
        "      for j in range(len(cur_pred)):\n",
        "        if (cur_pred[j] == 1 and cur_pred[j] == cur_true[j]):\n",
        "          true_pos += 1\n",
        "\n",
        "    if all_pos:\n",
        "        return true_pos/all_pos   \n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def recall(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Recall is True Positives / All Positive Labels\n",
        "    \"\"\"\n",
        "\n",
        "    false_neg = 0\n",
        "    true_pos = 0\n",
        "    for i in range(len(predicted_labels)):\n",
        "      cur_pred = predicted_labels[i]\n",
        "      cur_true = true_labels[i]\n",
        "    \n",
        "      for j in range(len(cur_pred)):\n",
        "        # Get true_pos\n",
        "        if (cur_pred[j] == 1 and cur_pred[j] == cur_true[j]):\n",
        "          true_pos += 1\n",
        "\n",
        "        # Get false_neg\n",
        "        if (cur_pred[j] == 0 and cur_true[j] == 1):\n",
        "          false_neg += 1\n",
        "      \n",
        "    denom = false_neg + true_pos\n",
        "    if denom:\n",
        "        return true_pos/denom\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "def f1_score(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int]\n",
        "):\n",
        "    \"\"\"\n",
        "    F1 score is the harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    P = precision(predicted_labels, true_labels)\n",
        "    R = recall(predicted_labels, true_labels)\n",
        "    if P and R:\n",
        "        return 2*P*R/(P+R)\n",
        "    else:\n",
        "        return 0.\n"
      ],
      "metadata": {
        "id": "aHcGkLzv0dsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "eCKKh-tLzWV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(\n",
        "    num_epochs,\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    dev_features,\n",
        "    dev_labels,\n",
        "    optimizer,\n",
        "    model,\n",
        "    possible_labels\n",
        "):\n",
        "    print(\"Training...\")\n",
        "    dev_f1_scores = []\n",
        "    #loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "    loss_func = torch.nn.BCELoss()\n",
        "\n",
        "    # Send the data to the device first\n",
        "    #train_features = train_features.to(device)\n",
        "    #train_labels = train_labels.to(device)\n",
        "    #dev_features = dev_features.to(device)\n",
        "    #dev_labels = dev_labels.to(device)\n",
        "\n",
        "    batches = list(zip(train_features, train_labels))\n",
        "    random.shuffle(batches)\n",
        "    for i in range(num_epochs):\n",
        "        losses = []\n",
        "        for features, labels in tqdm(batches):\n",
        "            # Empty the dynamic computation graph\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(features)\n",
        "            loss = loss_func(preds, labels.float())\n",
        "            \n",
        "            # Backpropogate the loss through our model\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
        "        # Estimate the f1 score for the development set\n",
        "        print(\"Evaluating dev...\")\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for sents, labels in tqdm(zip(dev_features, dev_labels), total=len(dev_features)):\n",
        "            pred = predict(model, sents)\n",
        "            all_preds.extend(pred.cpu().detach().numpy())\n",
        "            all_labels.extend(list(labels.cpu().numpy()))\n",
        "        dev_f1 = f1_score(all_preds, all_labels)\n",
        "        print(f\"Dev F1 {dev_f1}\")\n",
        "        dev_f1_scores.append(dev_f1)\n",
        "\n",
        "    # Print the best dev_f1 score for result reporting\n",
        "    print(f\"Best dev F1 score: {np.max(dev_f1_scores)}\")\n",
        "    print(f\"Best iteration: {np.argmax(dev_f1_scores)}\")\n",
        "    \n",
        "    # Return the trained model\n",
        "    return model"
      ],
      "metadata": {
        "id": "Xp81C1FczX_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Phase"
      ],
      "metadata": {
        "id": "J56VAQ-k1uF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "82BHsUDb2KYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of labels (should be 20)\n",
        "possible_labels = len(train_labels[0])\n",
        "if possible_labels != 20:\n",
        "  raise RuntimeError(f\"Instead of 20 possible labels, we found {possible_labels}.\")\n",
        "\n",
        "# Intialize model\n",
        "model = NLIClassifier(output_size=possible_labels, hidden_size=hidden_size, dropout_prob=dropout_prob)\n",
        "model.train()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "6Uvma7511u01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbcbeb4-60e5-46cd-b11a-60adfe4bec7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "W9vCMzYO3oVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training\n",
        "trained_model = training_loop(\n",
        "    epochs,\n",
        "    train_input_batches,\n",
        "    train_label_batches,\n",
        "    val_input_batches,\n",
        "    val_label_batches,\n",
        "    optimizer,\n",
        "    model,\n",
        "    list(range(possible_labels))\n",
        ")\n"
      ],
      "metadata": {
        "id": "hhmsgNFE3p1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c95a917-329d-4712-f57d-9f2f7bd0865a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss: 0.4154371694299101\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:13<00:00, 74.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.40482737549648634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:04<00:00, 27.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss: 0.38724998390401594\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:12<00:00, 85.07it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.44411326378539495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, loss: 0.3755833135761377\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:07<00:00, 130.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4629518072289157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, loss: 0.3671064178907234\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 128.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.47992836890016416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, loss: 0.3607089251052332\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 118.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4817649707339037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 41.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, loss: 0.35491062302625814\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.48995159160921226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, loss: 0.3517393542610052\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4916302216860202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 33.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, loss: 0.3477695795870919\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 109.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5072004608294931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8, loss: 0.3433566942014767\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 109.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5017880131597768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9, loss: 0.33880056110957196\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.512449575740715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10, loss: 0.3353435326623553\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 129.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.48707280832095096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11, loss: 0.33288778853780443\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5150226757369615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12, loss: 0.32915396476519926\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 109.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5134336756224503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13, loss: 0.3261782397295683\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5223126578725793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14, loss: 0.3218373917896329\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5201506066099568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15, loss: 0.31757387018385735\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:10<00:00, 104.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5290705898995615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16, loss: 0.3164207960358103\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.517804775869292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17, loss: 0.3141324629310433\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5249197263716321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18, loss: 0.31025453696724115\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5153168275283255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19, loss: 0.3097610908155223\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 106.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.523783185840708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20, loss: 0.3061353536962553\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5339710226473484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 21, loss: 0.3011023966410688\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:10<00:00, 95.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5221301110018266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 37.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 22, loss: 0.29938683277778044\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5279685966633956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23, loss: 0.2971792061820285\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:11<00:00, 89.51it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5260676650027731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 24, loss: 0.29518546141285934\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5333882934872217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25, loss: 0.29288141167800846\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5367535512343125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 26, loss: 0.2915513982982126\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5274360746371803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 27, loss: 0.28900965092746356\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 115.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5333704115684094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 41.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 28, loss: 0.286917103269628\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5369183616606297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 29, loss: 0.2839874719617931\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5321876704855428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30, loss: 0.2829232176069085\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5254260395364689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 31, loss: 0.28158858953086474\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:11<00:00, 92.07it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5434079441066115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 32, loss: 0.27707777473762746\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5438058407787704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 33, loss: 0.27409105810500284\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5327270221854761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 34, loss: 0.2719031510917285\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5365524683032102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 35, loss: 0.271244932449501\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.532153229152261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 36, loss: 0.2667283623955632\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5308839190628328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 37, loss: 0.26606133204835064\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5243404949687245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 38, loss: 0.2646480650847195\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5203408466190215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 39, loss: 0.26187098435773193\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 110.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5199565571544935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 40, loss: 0.25914471226794117\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 109.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5247829739568749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 41, loss: 0.2570255677436144\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5203744493392071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 42, loss: 0.25503725063709815\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5068068472840006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 43, loss: 0.25317987776894607\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 109.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5189131592967501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 44, loss: 0.25338777706368276\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5267750892502975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 45, loss: 0.24897098291011258\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5118358758548133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 46, loss: 0.24752998636424087\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5239608154620069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 41.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 47, loss: 0.2448996808237702\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 116.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5193021019370793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 48, loss: 0.24290464579604054\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 108.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5238158418468887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 49, loss: 0.2420847733284681\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 126.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5093286122838077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 50, loss: 0.2421200967017021\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 107.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5171460176991151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 51, loss: 0.23822504566370986\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 128.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5114879649890591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 52, loss: 0.23850060840144413\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5059782608695652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 53, loss: 0.2349952890445258\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.524261317430198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 54, loss: 0.23482007311500666\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 106.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5157694399129962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 55, loss: 0.23321060005945105\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5141678129298487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 56, loss: 0.2306307554244995\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 106.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5021160409556314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 57, loss: 0.22936987967891548\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 122.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5154413774255261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 38.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 58, loss: 0.22789861682717127\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5104818949087938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 59, loss: 0.22885856726242385\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4988558352402746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 60, loss: 0.22816718125161323\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.514872334824954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 61, loss: 0.22859140104464903\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 106.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.506378802747792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 62, loss: 0.22635813808168165\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5116215848851434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 63, loss: 0.22486095949438692\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5134876078323977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 64, loss: 0.22242121239199894\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 105.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5087209302325582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 42.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 65, loss: 0.22331988822867851\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 111.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5122143420015761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 66, loss: 0.220057746722498\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5105820105820106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 67, loss: 0.219449745789739\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5107442348008386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 68, loss: 0.2173777150971289\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 114.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5194123819517313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 69, loss: 0.21815186636593506\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5099399599733155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 70, loss: 0.2132463783935736\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5057354065765995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 71, loss: 0.21254878576475245\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5079281876556153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 72, loss: 0.21490926949577477\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:10<00:00, 96.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5103610061253747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 42.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 73, loss: 0.2134998714878359\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4998016660055533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 74, loss: 0.20928401926546605\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5045281533009581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 75, loss: 0.20761414907360806\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 128.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5098141847683851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 76, loss: 0.20570634095040896\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 112.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.50336012649888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 77, loss: 0.2061884141605319\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 127.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5008649367930804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 78, loss: 0.2060104486823992\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5027653410587306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 79, loss: 0.20369348805824308\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 80, loss: 0.20300785173441618\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:11<00:00, 90.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5001316829075585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 81, loss: 0.20044886929388264\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 122.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.503132915611252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 82, loss: 0.19994705540078286\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4995411039727284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 83, loss: 0.1974352420741365\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 121.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49271212909942735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 84, loss: 0.19905833956849484\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 119.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5043033889187736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 85, loss: 0.19856211332646945\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 117.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5026567481402763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 86, loss: 0.1970047934819724\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49240924092409244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 87, loss: 0.19783678707730679\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 125.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49812030075187974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 88, loss: 0.19627168063671535\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 105.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49456594659868514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 42.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 89, loss: 0.19283691964304175\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 110.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49642464246424645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 90, loss: 0.192386059986271\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49780380673499275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 44.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 91, loss: 0.19247474123275918\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.48854961832061067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 92, loss: 0.19399393514822458\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 106.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4980524539080758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 93, loss: 0.19178887399780842\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4969193678006965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 94, loss: 0.19033352808870432\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 123.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.5036428666048482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 95, loss: 0.18720071142866412\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 124.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4892885480031738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 40.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 96, loss: 0.189245545568357\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 119.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49765635462702557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:03<00:00, 43.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 97, loss: 0.18501822218185163\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 108.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.49617111169791395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 98, loss: 0.18337730460494528\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:08<00:00, 128.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4939695857367593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131/131 [00:02<00:00, 43.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 99, loss: 0.18248631276247154\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1044/1044 [00:09<00:00, 108.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev F1 0.4938205265986029\n",
            "Best dev F1 score: 0.5438058407787704\n",
            "Best iteration: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Model on Entire Dataset for Evaluation"
      ],
      "metadata": {
        "id": "MlQ0JsJbM3ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up our output DataFrame\n",
        "cols = [c for c in train_labs_df.columns if c != 'labels']\n",
        "out_df = pd.DataFrame(columns=cols)\n",
        "\n",
        "# Set the Arg ID as the index for easy access\n",
        "out_df['Argument ID'] = full_ids\n",
        "out_df.set_index('Argument ID', inplace = True)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "for sents, id in tqdm(zip(full_input_batches, full_ids), total=len(full_input_batches)):\n",
        "  # Get our prediction\n",
        "  pred = predict(model, sents).cpu().detach().numpy()[0]\n",
        "\n",
        "  # Add it to the output DataFrame\n",
        "  out_df.loc[id] = pred\n",
        "\n",
        "# Print out for error checking\n",
        "out_df.info()"
      ],
      "metadata": {
        "id": "hLV0_4NyNx3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278441c3-44b5-4d29-82f9-40ee8c34aab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5220/5220 [01:13<00:00, 70.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5220 entries, A01001 to D27100\n",
            "Data columns (total 20 columns):\n",
            " #   Column                      Non-Null Count  Dtype \n",
            "---  ------                      --------------  ----- \n",
            " 0   Self-direction: thought     5220 non-null   object\n",
            " 1   Self-direction: action      5220 non-null   object\n",
            " 2   Stimulation                 5220 non-null   object\n",
            " 3   Hedonism                    5220 non-null   object\n",
            " 4   Achievement                 5220 non-null   object\n",
            " 5   Power: dominance            5220 non-null   object\n",
            " 6   Power: resources            5220 non-null   object\n",
            " 7   Face                        5220 non-null   object\n",
            " 8   Security: personal          5220 non-null   object\n",
            " 9   Security: societal          5220 non-null   object\n",
            " 10  Tradition                   5220 non-null   object\n",
            " 11  Conformity: rules           5220 non-null   object\n",
            " 12  Conformity: interpersonal   5220 non-null   object\n",
            " 13  Humility                    5220 non-null   object\n",
            " 14  Benevolence: caring         5220 non-null   object\n",
            " 15  Benevolence: dependability  5220 non-null   object\n",
            " 16  Universalism: concern       5220 non-null   object\n",
            " 17  Universalism: nature        5220 non-null   object\n",
            " 18  Universalism: tolerance     5220 non-null   object\n",
            " 19  Universalism: objectivity   5220 non-null   object\n",
            "dtypes: object(20)\n",
            "memory usage: 985.4+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the output to a TSV\n",
        "out_df = out_df.astype(int)\n",
        "#out_df.to_csv('/content/drive/MyDrive/nlp_sp/data/model-preds.tsv', sep=\"\\t\")               # Spencer\n",
        "out_df.to_csv('/content/drive/MyDrive/csci5832_project/results/model-preds.tsv', sep=\"\\t\")  # Caroline\n",
        "print(\"Finished run!\")\n"
      ],
      "metadata": {
        "id": "5ZFoeVqEO7AL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da09383c-6a26-4cf2-f69e-53a506cfe84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished run!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test phase"
      ],
      "metadata": {
        "id": "9_vfz5A2zCxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set data preprocessing"
      ],
      "metadata": {
        "id": "kcKNlUA1zJUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "SiKDdcGc8W_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test arguments\n",
        "#train_args_df = pd.read_csv('/content/drive/MyDrive/nlp_sp/data/arguments-test.tsv', sep='\\t')         # Spencer\n",
        "test_args_df = pd.read_csv('/content/drive/MyDrive/csci5832_project/data/arguments-test.tsv', sep='\\t') # Caroline\n",
        "# view structure\n",
        "test_args_df.info()"
      ],
      "metadata": {
        "id": "p9ypdo5wzFmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data prep"
      ],
      "metadata": {
        "id": "JnFguuUq8gZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert each row to a dictionary -> List[Dict]\n",
        "test_data = test_args_df.to_dict(orient='records')\n",
        "# print examples\n",
        "print('test example:\\n', test_data[0])"
      ],
      "metadata": {
        "id": "jSmNPL_N8lNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "csMhgHAc8Y6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load samples from HuggingFace dataset to be batched and encoded\n",
        "# identically defined for train/dev section but here again for ease of use\n",
        "\n",
        "class BatchTokenizer:\n",
        "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
        "    \"\"\"HuggingFace docs: https://huggingface.co/transformers/v3.0.2/preprocessing.html\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer\n",
        "\n",
        "        Args:\n",
        "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
        "        \"\"\"\n",
        "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "    \n",
        "    # HuggingFace tokenizer will join data with sentence separator token\n",
        "    # and match batches of tokenized and encoded sentences\n",
        "    def get_sep_token(self,):\n",
        "        return self.hf_tokenizer.sep_token\n",
        "\n",
        "    # call method can only take a pair of inputs, but we have three\n",
        "    # conclusion batch, stance batch, and premise batch\n",
        "    # so we create a hack\n",
        "    #def __call__(self, con_batch: List[str], stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:\n",
        "\n",
        "    def __call__(self, con_stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:  \n",
        "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
        "\n",
        "        We return a dictionary of tensors per the huggingface model specification.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): A List of sentence strings\n",
        "\n",
        "        Returns:\n",
        "            Dict: The dictionary of token specifications provided by HuggingFace\n",
        "        \"\"\"\n",
        "        # The HF tokenizer will PAD for us, and additionally combine \n",
        "        # the two sentences deimited by the [SEP] token.\n",
        "        enc = self.hf_tokenizer(\n",
        "            con_stan_batch,\n",
        "            prem_batch,\n",
        "            #stan_batch,\n",
        "            #prem_batch,\n",
        "            padding=True,\n",
        "            return_token_type_ids=False, # ignore with hack\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return enc"
      ],
      "metadata": {
        "id": "bvxgL2eI8a6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = BatchTokenizer()"
      ],
      "metadata": {
        "id": "Y5E54vhc9CUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Batch\""
      ],
      "metadata": {
        "id": "G3m89CMi8jAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine another function to generate triple-wise inputs (test data w/o labels)\n",
        "\n",
        "def generate_triplewise_input_test(dataset: List[Dict]) -> (List[str], List[str], List[str], List[str]):\n",
        "    \"\"\"\n",
        "    group all argument components\n",
        "    a datapoint is now a dictionary of \n",
        "    argument id, conclusion, stance, premise\n",
        "    \"\"\"\n",
        "\n",
        "    # extract each observation from dictionary; save to list\n",
        "    d_vals = []\n",
        "    for i in range(len(dataset)):\n",
        "        d_vals.append(list(dataset[i].values()))\n",
        "\n",
        "    # store data items in lists by three categories by id\n",
        "    id_lst = []    \n",
        "    conclusion_lst = []\n",
        "    stance_lst = []\n",
        "    premise_lst = []\n",
        "\n",
        "    # generate separate lists from each observation\n",
        "    for i in range(len(d_vals)):\n",
        "        id_lst.append(d_vals[i][0])\n",
        "        conclusion_lst.append(d_vals[i][1])\n",
        "        stance_lst.append(d_vals[i][2])\n",
        "        premise_lst.append(d_vals[i][3])\n",
        "\n",
        "    # add [SEP] token before every stance in list\n",
        "    stance_lst = [' [SEP] ' + s for s in stance_lst]\n",
        "\n",
        "    return id_lst, conclusion_lst, stance_lst, premise_lst"
      ],
      "metadata": {
        "id": "8f_1CG0U8kZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to generate triple-wise inputs and labels for batching\n",
        "\n",
        "# test data\n",
        "test_ids, test_conclusions, test_stances, test_premises = generate_triplewise_input_test(test_data)"
      ],
      "metadata": {
        "id": "8rJpQUMS9L7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporarily combine conclusions and stances separate with [SEP]\n",
        "# use hack to merge tokenized conclusion batch, stance batch, and premise batch\n",
        "\n",
        "# test data\n",
        "test_conclusions_stances = []\n",
        "for i in range(len(test_conclusions)):\n",
        "  test_conclusions_stances.append(test_conclusions[i] + test_stances[i])"
      ],
      "metadata": {
        "id": "gxx2IlDK9L44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to chunk data for batches \n",
        "# identically defined for train/dev section but here again for ease of use\n",
        "\n",
        "# for train labels\n",
        "def chunk(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i: i+n]\n",
        "\n",
        "# for train features\n",
        "def chunk_multi(lst1, lst2, n):\n",
        "    for i in range(0, len(lst1), n):\n",
        "        yield lst1[i: i+n], lst2[i: i+n]"
      ],
      "metadata": {
        "id": "AUToyzpI9XGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch input data \n",
        "# tokenize and encode simultaneously since we are using HuggingFace\n",
        "\n",
        "# single \"batch\"\n",
        "test_size = 1\n",
        "test_input_batches = [b for b in chunk_multi(test_conclusions_stances, test_premises, test_size)]\n",
        "\n",
        "# tokenize + encode\n",
        "test_input_batches = [tokenizer(*batch).to(device) for batch in test_input_batches]"
      ],
      "metadata": {
        "id": "dX1fF8m79XDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check test data example\n",
        "print(test_input_batches[0])\n",
        "encoded_test_tst = tokenizer.hf_tokenizer.batch_decode(test_input_batches[0]['input_ids'])\n",
        "encoded_test_tst[0]"
      ],
      "metadata": {
        "id": "stu9aSaY9bzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set predictions \n",
        "Obtain the predictions on the test set using the trained model "
      ],
      "metadata": {
        "id": "Ji6L6z7Byidh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up our output DataFrame\n",
        "cols = [c for c in train_labs_df.columns if c != 'labels']\n",
        "out_df = pd.DataFrame(columns=cols)\n",
        "\n",
        "# Set the Arg ID as the index for easy access\n",
        "out_df['Argument ID'] = test_ids\n",
        "out_df.set_index('Argument ID', inplace = True)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "for sents, id in tqdm(zip(test_input_batches, test_ids), total=len(test_input_batches)):\n",
        "  # Get our prediction\n",
        "  pred = predict(model, sents).cpu().detach().numpy()[0]\n",
        "\n",
        "  # Add it to the output DataFrame\n",
        "  out_df.loc[id] = pred\n",
        "\n",
        "# Print out for error checking\n",
        "out_df.info()"
      ],
      "metadata": {
        "id": "R9QEH52UyjNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the output to a TSV\n",
        "out_df = out_df.astype(int)\n",
        "#out_df.to_csv('/content/drive/MyDrive/nlp_sp/data/model-preds-test.tsv', sep=\"\\t\")               # Spencer\n",
        "out_df.to_csv('/content/drive/MyDrive/csci5832_project/results/model-preds-test.tsv', sep=\"\\t\")   # Caroline\n",
        "print(\"Finished run!\")"
      ],
      "metadata": {
        "id": "70Kk1F66-m92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}