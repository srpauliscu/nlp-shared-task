{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JBDUhwxH0Xpm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90602079319d43f1a3ad88bb84eaf78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8758ad9d2664e2494204f8e0c263b86",
              "IPY_MODEL_3837788907044f849eee2bc1c0d59914",
              "IPY_MODEL_7fb6ad9e0ce94ed7b3b67dc01d676cad"
            ],
            "layout": "IPY_MODEL_f1cd900d75694ac5b89ed7f877ce5469"
          }
        },
        "c8758ad9d2664e2494204f8e0c263b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04a99ad0184c48b3b71f2caee55bfa0e",
            "placeholder": "​",
            "style": "IPY_MODEL_de1c3613deb34a758ca0997dcaa595bf",
            "value": "Downloading: 100%"
          }
        },
        "3837788907044f849eee2bc1c0d59914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc7b78c72952465982544f96cb3e47b3",
            "max": 286,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33428e4631974d199d05aa0328443520",
            "value": 286
          }
        },
        "7fb6ad9e0ce94ed7b3b67dc01d676cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563e07182cbc47f98f1a3c47701e33b7",
            "placeholder": "​",
            "style": "IPY_MODEL_973153acb6044abf90be4ef46af4b036",
            "value": " 286/286 [00:00&lt;00:00, 5.23kB/s]"
          }
        },
        "f1cd900d75694ac5b89ed7f877ce5469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04a99ad0184c48b3b71f2caee55bfa0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1c3613deb34a758ca0997dcaa595bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc7b78c72952465982544f96cb3e47b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33428e4631974d199d05aa0328443520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "563e07182cbc47f98f1a3c47701e33b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "973153acb6044abf90be4ef46af4b036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a94a1d2a9b3041cab35bf01b26155a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e34219c7a69f435b8a14bdb8a739701d",
              "IPY_MODEL_a57799e5575d49b58d2c8a2349c344a3",
              "IPY_MODEL_85676b3dbcea44ecb4275690857692a3"
            ],
            "layout": "IPY_MODEL_4dfba4e7c4e3489899f302c76ab4df68"
          }
        },
        "e34219c7a69f435b8a14bdb8a739701d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d8861b2ea1c459ea0c1e35022ba0c9a",
            "placeholder": "​",
            "style": "IPY_MODEL_092742671dab43bb9779359fbea3f61d",
            "value": "Downloading: 100%"
          }
        },
        "a57799e5575d49b58d2c8a2349c344a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_823055fa6f1649198105727b122ce7a9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a0a510347a44016be9fa3df6f6883b6",
            "value": 231508
          }
        },
        "85676b3dbcea44ecb4275690857692a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce29597d58d40cfb6abc2fec2a9096d",
            "placeholder": "​",
            "style": "IPY_MODEL_652abd3a70024a948b5102ce7dedf4b8",
            "value": " 232k/232k [00:00&lt;00:00, 531kB/s]"
          }
        },
        "4dfba4e7c4e3489899f302c76ab4df68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d8861b2ea1c459ea0c1e35022ba0c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "092742671dab43bb9779359fbea3f61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "823055fa6f1649198105727b122ce7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a0a510347a44016be9fa3df6f6883b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ce29597d58d40cfb6abc2fec2a9096d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "652abd3a70024a948b5102ce7dedf4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58d316da58e446f29b25202fcf864f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7079482775a94647a65112b48e54849d",
              "IPY_MODEL_4cb0b19fc6f6478190a1c6e8e53fd401",
              "IPY_MODEL_3108f52246fd4b92849d738011629260"
            ],
            "layout": "IPY_MODEL_a6b5b13936354e44ae32db7597cca686"
          }
        },
        "7079482775a94647a65112b48e54849d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90b1f6e3b5cf4d57a9a3e3c1f89ca585",
            "placeholder": "​",
            "style": "IPY_MODEL_5017943ca990447b8dbfd77cc50fc048",
            "value": "Downloading: 100%"
          }
        },
        "4cb0b19fc6f6478190a1c6e8e53fd401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb5eb5d52004f6abfcbe7f4fb2b243f",
            "max": 116270890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_445bf3265a4c441092acbd1afe41e62a",
            "value": 116270890
          }
        },
        "3108f52246fd4b92849d738011629260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79ff8b223244c87964ef09964e970a6",
            "placeholder": "​",
            "style": "IPY_MODEL_e5edf81dc05f41098b9cd89b2be0e00b",
            "value": " 116M/116M [00:03&lt;00:00, 58.6MB/s]"
          }
        },
        "a6b5b13936354e44ae32db7597cca686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b1f6e3b5cf4d57a9a3e3c1f89ca585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5017943ca990447b8dbfd77cc50fc048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eb5eb5d52004f6abfcbe7f4fb2b243f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445bf3265a4c441092acbd1afe41e62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b79ff8b223244c87964ef09964e970a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5edf81dc05f41098b9cd89b2be0e00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script Setup"
      ],
      "metadata": {
        "id": "02Eyvjuw9cbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrbPbw-W_xsp",
        "outputId": "e6c515cc-2fe5-465c-af76-56d9a75d85cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -r drive/MyDrive/nlp_sp/env/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jnTL9B6_i_K",
        "outputId": "39c8f626-28eb-4710-9cd1-ce520c2165c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 2)) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 3)) (1.12.1+cu113)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (4.25.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from -r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (0.11.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2022.11.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (4.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets->-r drive/MyDrive/nlp_sp/env/requirements.txt (line 5)) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import block\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy import logical_and, sum as t_sum\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "j4MvReNO9e8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup for CUDA\n",
        "\n",
        "'''\n",
        "\n",
        "Important: Every tensor, layer, and model needs to be sent to the same device using to()\n",
        "Ex: \n",
        "  ten = torch.ones(4,5).to(device)\n",
        "\n",
        "'''\n",
        "\n",
        "# Get the best device to run on\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ],
      "metadata": {
        "id": "NErpy8X6-NUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d57a44-794d-4738-b04c-edae8a7d2227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "JLflCMhp4IVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define hyparameters near top to make changing them easier"
      ],
      "metadata": {
        "id": "cGpJThBt4QHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training loops\n",
        "epochs = 20\n",
        "\n",
        "# Learning rate - should be very small when using Adam\n",
        "LR = .0001\n",
        "\n",
        "# Dropout probability\n",
        "dropout_prob = 0.2\n",
        "\n",
        "# Batch size\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "jfFOC_WZ4Hm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "RuNSOLZz1VXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Format\n",
        "\n",
        "**Data:** `arguments-training/validation/testing.tsv`\n",
        "(5220 arguments)\n",
        "- Argument ID\n",
        "- Conclusion \n",
        "- Stance (e.g., in favor, against)\n",
        "- Premise (justification for conclusion)\n",
        "\n",
        "**Labels:** `labels-training/validation/testing.tsv` \n",
        "(20 binary value labels per argument)\n",
        "- Argument ID\n",
        "- Self-direction: thought\n",
        "- Self-direction: action\n",
        "- Stimulation\n",
        "- Hedonism\n",
        "- Achievement\n",
        "- Power: dominance\n",
        "- Power: resources\n",
        "- Face\n",
        "- Security: personal\n",
        "- Security: societal\n",
        "- Tradition\n",
        "- Conformity: rules\n",
        "- Conformity: interpersonal\n",
        "- Humility\n",
        "- Benevolence: caring\n",
        "- Benevolence: dependability\n",
        "- Universalism: concern\n",
        "- Universalism: nature\n",
        "- Universalism: tolerance\n",
        "- Universalism: objectivity\n",
        "\n",
        "**Access:** https://doi.org/10.5281/zenodo.6814563"
      ],
      "metadata": {
        "id": "I3ZSPKuiyXxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "jwPoXtFRzPub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training arguments\n",
        "train_args_df = pd.read_csv('/content/drive/MyDrive/nlp_sp/data/arguments-training.tsv', sep='\\t')\n",
        "# view structure\n",
        "train_args_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP428yMAz6GZ",
        "outputId": "9b6c9dc4-572d-4007-ead9-d5bed8b0f7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 163.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training labels\n",
        "train_labs_df = pd.read_csv('/content/drive/MyDrive/nlp_sp/data/labels-training.tsv', sep='\\t')\n",
        "# view structure\n",
        "train_labs_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKlU8wD61Jfq",
        "outputId": "63903efd-d3b4-4270-b972-684316ab2da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5220 entries, 0 to 5219\n",
            "Data columns (total 21 columns):\n",
            " #   Column                      Non-Null Count  Dtype \n",
            "---  ------                      --------------  ----- \n",
            " 0   Argument ID                 5220 non-null   object\n",
            " 1   Self-direction: thought     5220 non-null   int64 \n",
            " 2   Self-direction: action      5220 non-null   int64 \n",
            " 3   Stimulation                 5220 non-null   int64 \n",
            " 4   Hedonism                    5220 non-null   int64 \n",
            " 5   Achievement                 5220 non-null   int64 \n",
            " 6   Power: dominance            5220 non-null   int64 \n",
            " 7   Power: resources            5220 non-null   int64 \n",
            " 8   Face                        5220 non-null   int64 \n",
            " 9   Security: personal          5220 non-null   int64 \n",
            " 10  Security: societal          5220 non-null   int64 \n",
            " 11  Tradition                   5220 non-null   int64 \n",
            " 12  Conformity: rules           5220 non-null   int64 \n",
            " 13  Conformity: interpersonal   5220 non-null   int64 \n",
            " 14  Humility                    5220 non-null   int64 \n",
            " 15  Benevolence: caring         5220 non-null   int64 \n",
            " 16  Benevolence: dependability  5220 non-null   int64 \n",
            " 17  Universalism: concern       5220 non-null   int64 \n",
            " 18  Universalism: nature        5220 non-null   int64 \n",
            " 19  Universalism: tolerance     5220 non-null   int64 \n",
            " 20  Universalism: objectivity   5220 non-null   int64 \n",
            "dtypes: int64(20), object(1)\n",
            "memory usage: 856.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "whsRrGx71cUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert multiple label columns to one label list column\n",
        "train_labs_df['labels'] = train_labs_df.loc[:, 'Self-direction: thought':'Universalism: objectivity'].values.tolist()"
      ],
      "metadata": {
        "id": "33okLWjn1poE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label distribution for full training data\n",
        "print('Self-direction: thought =', sum(train_labs_df['Self-direction: thought']))\n",
        "print('Self-direction: action =', sum(train_labs_df['Self-direction: action']))\n",
        "print('Stimulation =', sum(train_labs_df['Stimulation']))\n",
        "print('Hedonism =', sum(train_labs_df['Hedonism']))\n",
        "print('Achievement = ', sum(train_labs_df['Achievement']))\n",
        "print('Power: dominance =', sum(train_labs_df['Power: dominance']))\n",
        "print('Power: resources =', sum(train_labs_df['Power: resources']))\n",
        "print('Face =', sum(train_labs_df['Face']))\n",
        "print('Security: personal =', sum(train_labs_df['Security: personal']))\n",
        "print('Security: societal =', sum(train_labs_df['Security: societal']))\n",
        "print('Tradition =', sum(train_labs_df['Tradition']))\n",
        "print('Conformity: rules =', sum(train_labs_df['Conformity: rules']))\n",
        "print('Conformity: interpersonal =', sum(train_labs_df['Conformity: interpersonal']))\n",
        "print('Humility =', sum(train_labs_df['Humility']))\n",
        "print('Benevolence: caring =', sum(train_labs_df['Benevolence: caring']))\n",
        "print('Benevolence: dependability =', sum(train_labs_df['Benevolence: dependability']))\n",
        "print('Universalism: concern =', sum(train_labs_df['Universalism: concern']))\n",
        "print('Universalism: nature =', sum(train_labs_df['Universalism: nature']))\n",
        "print('Universalism: tolerance =', sum(train_labs_df['Universalism: tolerance']))\n",
        "print('Universalism: objectivity =', sum(train_labs_df['Universalism: objectivity']))\n",
        "\n",
        "print('\\nTotal number of samples = ', len(train_labs_df))"
      ],
      "metadata": {
        "id": "P9jaFRCB3clx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine dfs to add label list to data dictionary\n",
        "train_merged_df = pd.merge(train_args_df, train_labs_df, on='Argument ID')\n",
        "train_merged_df = train_merged_df.drop(columns=['Self-direction: thought',\n",
        "                                                'Self-direction: action',\n",
        "                                                'Stimulation',\n",
        "                                                'Hedonism',\n",
        "                                                'Achievement',\n",
        "                                                'Power: dominance',\n",
        "                                                'Power: resources',\n",
        "                                                'Face',\n",
        "                                                'Security: personal',\n",
        "                                                'Security: societal',\n",
        "                                                'Tradition',\n",
        "                                                'Conformity: rules',\n",
        "                                                'Conformity: interpersonal',\n",
        "                                                'Humility',\n",
        "                                                'Benevolence: caring',\n",
        "                                                'Benevolence: dependability',\n",
        "                                                'Universalism: concern',\n",
        "                                                'Universalism: nature',\n",
        "                                                'Universalism: tolerance',\n",
        "                                                'Universalism: objectivity'])"
      ],
      "metadata": {
        "id": "tH2O2t-d1r4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view structure\n",
        "train_merged_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ACBME6l1xph",
        "outputId": "f8ca78d8-1674-49ad-8d9c-00048118c123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5220 entries, 0 to 5219\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Argument ID  5220 non-null   object\n",
            " 1   Conclusion   5220 non-null   object\n",
            " 2   Stance       5220 non-null   object\n",
            " 3   Premise      5220 non-null   object\n",
            " 4   labels       5220 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 244.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val Split"
      ],
      "metadata": {
        "id": "ylBInj2l3f82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train data into 80/20 train/val\n",
        "train_data, val_data = train_test_split(train_merged_df, test_size=0.2, random_state=4)"
      ],
      "metadata": {
        "id": "j40FHbSy1Jo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert each row to a dictionary -> List[Dict]\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "val_data = val_data.to_dict(orient='records')\n",
        "# print examples\n",
        "print('training example:\\n', train_data[0])\n",
        "print('validation example:\\n', val_data[0])"
      ],
      "metadata": {
        "id": "RMELWMrk1JmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "8rX8q7I22cmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load samples from HuggingFace dataset to be batched and encoded\n",
        "\n",
        "class BatchTokenizer:\n",
        "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
        "    \"\"\"HuggingFace docs: https://huggingface.co/transformers/v3.0.2/preprocessing.html\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer\n",
        "\n",
        "        Args:\n",
        "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
        "        \"\"\"\n",
        "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
        "    \n",
        "    # HuggingFace tokenizer will join data with sentence separator token\n",
        "    # and match batches of tokenized and encoded sentences\n",
        "    def get_sep_token(self,):\n",
        "        return self.hf_tokenizer.sep_token\n",
        "\n",
        "    # call method can only take a pair of inputs, but we have three\n",
        "    # conclusion batch, stance batch, and premise batch\n",
        "    # so we create a hack\n",
        "    #def __call__(self, con_batch: List[str], stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:\n",
        "\n",
        "    def __call__(self, con_stan_batch: List[str], prem_batch: List[str]) -> List[List[str]]:  \n",
        "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
        "\n",
        "        We return a dictionary of tensors per the huggingface model specification.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): A List of sentence strings\n",
        "\n",
        "        Returns:\n",
        "            Dict: The dictionary of token specifications provided by HuggingFace\n",
        "        \"\"\"\n",
        "        # The HF tokenizer will PAD for us, and additionally combine \n",
        "        # the two sentences deimited by the [SEP] token.\n",
        "        enc = self.hf_tokenizer(\n",
        "            con_stan_batch,\n",
        "            prem_batch,\n",
        "            #stan_batch,\n",
        "            #prem_batch,\n",
        "            padding=True,\n",
        "            return_token_type_ids=False, # ignore with hack\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return enc"
      ],
      "metadata": {
        "id": "AIy5AQQy2gzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = BatchTokenizer()"
      ],
      "metadata": {
        "id": "lg4r_uFi2oHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "90602079319d43f1a3ad88bb84eaf78b",
            "c8758ad9d2664e2494204f8e0c263b86",
            "3837788907044f849eee2bc1c0d59914",
            "7fb6ad9e0ce94ed7b3b67dc01d676cad",
            "f1cd900d75694ac5b89ed7f877ce5469",
            "04a99ad0184c48b3b71f2caee55bfa0e",
            "de1c3613deb34a758ca0997dcaa595bf",
            "dc7b78c72952465982544f96cb3e47b3",
            "33428e4631974d199d05aa0328443520",
            "563e07182cbc47f98f1a3c47701e33b7",
            "973153acb6044abf90be4ef46af4b036",
            "a94a1d2a9b3041cab35bf01b26155a9e",
            "e34219c7a69f435b8a14bdb8a739701d",
            "a57799e5575d49b58d2c8a2349c344a3",
            "85676b3dbcea44ecb4275690857692a3",
            "4dfba4e7c4e3489899f302c76ab4df68",
            "7d8861b2ea1c459ea0c1e35022ba0c9a",
            "092742671dab43bb9779359fbea3f61d",
            "823055fa6f1649198105727b122ce7a9",
            "9a0a510347a44016be9fa3df6f6883b6",
            "8ce29597d58d40cfb6abc2fec2a9096d",
            "652abd3a70024a948b5102ce7dedf4b8"
          ]
        },
        "outputId": "11f79941-1be9-4f41-8450-ed01fbe23ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/286 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90602079319d43f1a3ad88bb84eaf78b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a94a1d2a9b3041cab35bf01b26155a9e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer without triplet hack (only two input types acceptable)\n",
        "token_ex = tokenizer(*[['this is the conclusion with more words', 'this is also a conclusion'], ['this is the premise', 'this is the second premise']])\n",
        "print(f\"{token_ex}\\n\")\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAUbG6kb2rLs",
        "outputId": "4b84f07e-268a-4227-8e51-e23f701b0855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  2023,\n",
            "          2003,  1996, 18458,   102],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2023,  2003,  1996,\n",
            "          2117, 18458,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] this is the premise [SEP]',\n",
              " '[CLS] this is also a conclusion [SEP] this is the second premise [SEP] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of use case for batch tokenizer with triplet hack\n",
        "token_ex2 = tokenizer(*[['this is the conclusion with more words [SEP] and a stance against', 'this is also a conclusion [SEP] with another stance that is in favor of'], ['this is the premise', 'this is the second premise']])\n",
        "print(f\"{token_ex2}\\n\")\n",
        "tokenizer.hf_tokenizer.batch_decode(token_ex2['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbJQZe7286L",
        "outputId": "e0568d67-df7d-40d2-dda2-acfb75c53da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996,  7091,  2007,  2062,  2616,   102,  1998,\n",
            "          1037, 11032,  2114,   102,  2023,  2003,  1996, 18458,   102,     0,\n",
            "             0,     0],\n",
            "        [  101,  2023,  2003,  2036,  1037,  7091,   102,  2007,  2178, 11032,\n",
            "          2008,  2003,  1999,  5684,  1997,   102,  2023,  2003,  1996,  2117,\n",
            "         18458,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the conclusion with more words [SEP] and a stance against [SEP] this is the premise [SEP] [PAD] [PAD] [PAD]',\n",
              " '[CLS] this is also a conclusion [SEP] with another stance that is in favor of [SEP] this is the second premise [SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "iA1nphB83Yln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate triple-wise inputs\n",
        "\n",
        "def generate_triplewise_input(dataset: List[Dict]) -> (List[str], List[str], List[str], List[List[int]]):\n",
        "    \"\"\"\n",
        "    TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
        "    a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
        "    \"\"\"\n",
        "\n",
        "    # extract each observation from dictionary; save to list\n",
        "    d_vals = []\n",
        "    for i in range(len(dataset)):\n",
        "        d_vals.append(list(dataset[i].values()))\n",
        "\n",
        "    # store data items in lists by three categories    \n",
        "    conclusion_lst = []\n",
        "    stance_lst = []\n",
        "    premise_lst = []\n",
        "\n",
        "    # store labels in list of lists of 20 labels\n",
        "    label_lst = []\n",
        "\n",
        "    # generate separate lists from each observation\n",
        "    for i in range(len(d_vals)):\n",
        "        conclusion_lst.append(d_vals[i][1])\n",
        "        stance_lst.append(d_vals[i][2])\n",
        "        premise_lst.append(d_vals[i][3])\n",
        "        label_lst.append(d_vals[i][4])\n",
        "\n",
        "    # add [SEP] token before every stance in list\n",
        "    stance_lst = [' [SEP] ' + s for s in stance_lst]\n",
        "\n",
        "    return conclusion_lst, stance_lst, premise_lst, label_lst"
      ],
      "metadata": {
        "id": "gYrXwQOR3f7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to generate triple-wise inputs and labels for batching\n",
        "\n",
        "# training data\n",
        "train_conclusions, train_stances, train_premises, train_labels = generate_triplewise_input(train_data)\n",
        "\n",
        "# validation data\n",
        "val_conclusions, val_stances, val_premises, val_labels = generate_triplewise_input(val_data)"
      ],
      "metadata": {
        "id": "KfqwdYsf3zof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporarily combine conclusions and stances separate with [SEP]\n",
        "# use hack to merge tokenized conclusion batch, stance batch, and premise batch\n",
        "\n",
        "# training data\n",
        "train_conclusions_stances = []\n",
        "for i in range(len(train_conclusions)):\n",
        "  train_conclusions_stances.append(train_conclusions[i] + train_stances[i])\n",
        "\n",
        "# validation data\n",
        "val_conclusions_stances = []\n",
        "for i in range(len(val_conclusions)):\n",
        "  val_conclusions_stances.append(val_conclusions[i] + val_stances[i])  "
      ],
      "metadata": {
        "id": "9htFdXaY33np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to chunk data for batches\n",
        "\n",
        "# for train labels\n",
        "def chunk(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i: i+n]\n",
        "\n",
        "# for train features\n",
        "def chunk_multi(lst1, lst2, n):\n",
        "    for i in range(0, len(lst1), n):\n",
        "        yield lst1[i: i+n], lst2[i: i+n]"
      ],
      "metadata": {
        "id": "tRfM1M-C4vvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch input data \n",
        "# tokenize and encode simultaneously since we are using HuggingFace\n",
        "\n",
        "# batch\n",
        "train_input_batches = [b for b in chunk_multi(train_conclusions_stances, train_premises, batch_size)]\n",
        "val_input_batches = [b for b in chunk_multi(val_conclusions_stances, val_premises, val_size)]\n",
        "\n",
        "# tokenize + encode\n",
        "train_input_batches = [tokenizer(*batch).to(device) for batch in train_input_batches]\n",
        "val_input_batches = [tokenizer(*batch).to(device) for batch in val_input_batches]"
      ],
      "metadata": {
        "id": "Nx_Gt6Iv4ywV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check training data example\n",
        "print(train_input_batches[0])\n",
        "encoded_tst = tokenizer.hf_tokenizer.batch_decode(train_input_batches[0]['input_ids'])\n",
        "encoded_tst[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "g3aH-j365Ync",
        "outputId": "c43d7f1d-1826-406f-9675-202d6a43edbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 4372, 6494,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2323,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2323,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2057, 2323,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2323,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2323,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] entrapment should be legalized [SEP] in favor of [SEP] if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to batch class labels\n",
        "# a single observation's label is a list of 20 labels\n",
        "\n",
        "def encode_labels(labels: List[List[int]]) -> torch.FloatTensor:\n",
        "    \"\"\"Turns the batch of labels into a tensor\n",
        "\n",
        "    Args:\n",
        "        labels (List[List[int]]): List of all lists of labels in batch\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor: Tensor of all lists of labels in batch\n",
        "    \"\"\"\n",
        "    \n",
        "    return torch.LongTensor(labels)\n"
      ],
      "metadata": {
        "id": "8KWtgfqQ5d3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply function to batch labels in same order as inputs\n",
        "# batch\n",
        "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
        "val_label_batches = [b for b in chunk(val_labels, batch_size)]\n",
        "# tokenize + encode\n",
        "train_label_batches = [encode_labels(batch).to(device) for batch in train_label_batches]\n",
        "val_label_batches = [encode_labels(batch).to(device) for batch in val_label_batches]"
      ],
      "metadata": {
        "id": "KysnHWFM5haJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Below is the code to define our model as well as the training loop."
      ],
      "metadata": {
        "id": "wp873QUQ8nEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Make Predictions"
      ],
      "metadata": {
        "id": "yTTeQ4A9jDTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(logits: torch.Tensor) -> torch.Tensor:\n",
        "  # This is equivalent to a threshold of 0.5\n",
        "  return torch.round(logits)\n",
        "\n",
        "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
        "    logits = model(sents)\n",
        "    return make_prediction(logits)"
      ],
      "metadata": {
        "id": "CotfrOnqjByn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "VA8Wt2R_8xc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to initialize weights for the chain classifiers\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "class NLIClassifier(torch.nn.Module):\n",
        "    def __init__(self, output_size: int, hidden_size: int, dropout_prob: float):\n",
        "      \n",
        "      # Basic initialization\n",
        "      super().__init__()\n",
        "      self.output_size = output_size\n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "      # Additional args\n",
        "      self.dropout_prob = dropout_prob\n",
        "\n",
        "      # Initialize BERT, which we use instead of a single embedding layer.\n",
        "      self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\").to(device)\n",
        "      \n",
        "      # Comment out these lines to unfreeze BERT params\n",
        "      for param in self.bert.parameters():\n",
        "          param.requires_grad = False\n",
        "          \n",
        "      # Get BERT's hiddem dim\n",
        "      self.bert_hidden_dimension = self.bert.config.hidden_size\n",
        "      \n",
        "      \n",
        "      # Single linear layer to project to hidden size\n",
        "      self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size).to(device)\n",
        "      \n",
        "      # Use RELU regularization\n",
        "      # TODO: Could try others\n",
        "      self.relu = torch.nn.ReLU()\n",
        "\n",
        "      '''\n",
        "\n",
        "      We are doing multi-label classification using a chain classifier.\n",
        "      For details, see: https://en.wikipedia.org/wiki/Multi-label_classification\n",
        "\n",
        "      Setup a classifier chain for the 20 labels.\n",
        "      To simplify code, just store them in a list and run through them sequentially.\n",
        "      They will be interpreted in the same order as the training data:\n",
        "\n",
        "      Self-direction: thought\n",
        "      Self-direction: action\n",
        "      Stimulation\n",
        "      Hedonism\n",
        "      Achievement\n",
        "      Power: dominance\n",
        "      Power: resources\n",
        "      Face\n",
        "      Security: personal\n",
        "      Security: societal\n",
        "      Tradition\n",
        "      Conformity: rules\n",
        "      Conformity: interpersonal\n",
        "      Humility\n",
        "      Benevolence: caring\n",
        "      Benevolence: dependability\n",
        "      Universalism: concern\n",
        "      Universalism: nature\n",
        "      Universalism: tolerance\n",
        "      Universalism: objectivity\n",
        "\n",
        "      '''\n",
        "\n",
        "      self.chain = []\n",
        "      for i in range(self.output_size):\n",
        "\n",
        "        # To make it a chain, the prediction from the previous classifier is \n",
        "        # appended to the input and used as the input for the next classifier\n",
        "\n",
        "        # Initialize each chain classifier\n",
        "        t = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(in_features=self.hidden_size + i, out_features = 1),\n",
        "            #nn.LogSoftmax(dim=2)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.chain.append(t.to(device))\n",
        "        # Initialize the weights\n",
        "        for c in self.chain:\n",
        "          c.apply(init_weights)\n",
        "\n",
        "    def encode_text(\n",
        "        self,\n",
        "        symbols: Dict\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Use BERT to create contextulized embeddings and get the output \n",
        "            from the pooling layer (i.e. embedding for CLR)\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoding of CLR for the given input\n",
        "        \"\"\"\n",
        "\n",
        "        # Run through BERT for contextualized embeddings\n",
        "        encoded_sequence = self.bert(**symbols)\n",
        "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
        "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "        #      and check the returns for the forward method.\n",
        "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
        "        \n",
        "        # Pooler output is initially (batch_size, bert_hidden_dimension)\n",
        "        pool_out = torch.unsqueeze(encoded_sequence['pooler_output'], dim=1)\n",
        "        return pool_out\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        symbols: Dict,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: _description_\n",
        "        \"\"\"\n",
        "        encoded_sents = self.encode_text(symbols)\n",
        "        output = self.hidden_layer(encoded_sents)\n",
        "        output = self.relu(output)\n",
        "        \n",
        "        # output is of size (batch_size, hidden_layer)\n",
        "\n",
        "        # Run through the classifier chain\n",
        "\n",
        "        cur_input = output\n",
        "        logits = []\n",
        "\n",
        "        for classifier in self.chain:\n",
        "\n",
        "          # Get output of next in chain\n",
        "          o = classifier(cur_input)\n",
        "\n",
        "          # Save the logits for training\n",
        "          logits.append(o)\n",
        "\n",
        "          # Make a prediction so we can append it to the next input\n",
        "          # TODO: Could also append raw logits, potentially\n",
        "          pred = make_prediction(o)\n",
        "\n",
        "          # Append the previous prediction to the input for the next classifier\n",
        "          cur_input = torch.cat([cur_input, pred], dim=2)\n",
        "\n",
        "        # Preds contains 20 tensors, each batch_size x 1 x 1\n",
        "        # We need to return one tensor that is 128 x 20\n",
        "        stack = logits[0].squeeze(dim=1)\n",
        "        for logit in logits[1:]:\n",
        "          stack = torch.cat([stack, logit.squeeze(dim=1)], dim=-1)\n",
        "        \n",
        "        return stack"
      ],
      "metadata": {
        "id": "4XXLNXdp8oOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "7A0BDNUl0TZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Functions"
      ],
      "metadata": {
        "id": "JBDUhwxH0Xpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Precision is True Positives / All Positives Predictions\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(pred_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "        \n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def recall(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Recall is True Positives / All Positive Labels\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(true_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def f1_score(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    which_label: int\n",
        "):\n",
        "    \"\"\"\n",
        "    F1 score is the harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
        "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
        "    if P and R:\n",
        "        return 2*P*R/(P+R)\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def macro_f1(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    possible_labels: List[int]\n",
        "):\n",
        "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
        "    # Macro, so we take the uniform avg.\n",
        "    return sum(scores) / len(scores)"
      ],
      "metadata": {
        "id": "aHcGkLzv0dsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "eCKKh-tLzWV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(\n",
        "    num_epochs,\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    dev_features,\n",
        "    dev_labels,\n",
        "    optimizer,\n",
        "    model,\n",
        "    possible_labels\n",
        "):\n",
        "    print(\"Training...\")\n",
        "    dev_f1_scores = []\n",
        "    #loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "    loss_func = torch.nn.BCELoss()\n",
        "\n",
        "    # Send the data to the device first\n",
        "    #train_features = train_features.to(device)\n",
        "    #train_labels = train_labels.to(device)\n",
        "    #dev_features = dev_features.to(device)\n",
        "    #dev_labels = dev_labels.to(device)\n",
        "\n",
        "    batches = list(zip(train_features, train_labels))\n",
        "    random.shuffle(batches)\n",
        "    for i in range(num_epochs):\n",
        "        losses = []\n",
        "        for features, labels in tqdm(batches):\n",
        "            # Empty the dynamic computation graph\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(features)\n",
        "            loss = loss_func(preds, labels.float())\n",
        "            \n",
        "            # Backpropogate the loss through our model\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
        "        # Estimate the f1 score for the development set\n",
        "        print(\"Evaluating dev...\")\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for sents, labels in tqdm(zip(dev_features, dev_labels), total=len(dev_features)):\n",
        "            pred = predict(model, sents)\n",
        "            all_preds.extend(pred)\n",
        "            all_labels.extend(list(labels.numpy()))\n",
        "\n",
        "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels)\n",
        "        print(f\"Dev F1 {dev_f1}\")\n",
        "        dev_f1_scores.append(dev_f1)\n",
        "        #print(all_preds)\n",
        "        \n",
        "    # Print the best dev_f1 score for result reporting\n",
        "    print(f\"Best dev F1 score: {np.max(dev_f1_scores)}\")\n",
        "    # Return the trained model\n",
        "    return model"
      ],
      "metadata": {
        "id": "Xp81C1FczX_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Phase"
      ],
      "metadata": {
        "id": "J56VAQ-k1uF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "82BHsUDb2KYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of labels (should be 20)\n",
        "possible_labels = len(train_labels[0])\n",
        "if possible_labels != 20:\n",
        "  raise RuntimeError(f\"Instead of 20 possible labels, we found {possible_labels}.\")\n",
        "\n",
        "# Intialize model\n",
        "model = NLIClassifier(output_size=possible_labels, hidden_size = 512, dropout_prob=dropout_prob)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
        "\n",
        "# Setup the validation set\n",
        "'''\n",
        "validation_input_batches = [b for b in chunk_multi(validation_premises, validation_hypotheses, batch_size)]\n",
        "# Tokenize + encode\n",
        "validation_input_batches = [tokenizer(*batch) for batch in validation_input_batches]\n",
        "validation_batch_labels = [b for b in chunk(validation_labels, batch_size)]\n",
        "validation_batch_labels = [encode_labels(batch) for batch in validation_batch_labels]\n",
        "'''\n",
        "\n",
        "# TODO: temp solution for testing\n",
        "validation_input_batches = train_input_batches\n",
        "validation_label_batches = train_label_batches\n"
      ],
      "metadata": {
        "id": "6Uvma7511u01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "58d316da58e446f29b25202fcf864f58",
            "7079482775a94647a65112b48e54849d",
            "4cb0b19fc6f6478190a1c6e8e53fd401",
            "3108f52246fd4b92849d738011629260",
            "a6b5b13936354e44ae32db7597cca686",
            "90b1f6e3b5cf4d57a9a3e3c1f89ca585",
            "5017943ca990447b8dbfd77cc50fc048",
            "7eb5eb5d52004f6abfcbe7f4fb2b243f",
            "445bf3265a4c441092acbd1afe41e62a",
            "b79ff8b223244c87964ef09964e970a6",
            "e5edf81dc05f41098b9cd89b2be0e00b"
          ]
        },
        "outputId": "62568f9a-7e5f-42b3-d20c-cf68d2b69aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/116M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58d316da58e446f29b25202fcf864f58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "W9vCMzYO3oVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training\n",
        "trained_model = training_loop(\n",
        "    epochs,\n",
        "    train_input_batches,\n",
        "    train_label_batches,\n",
        "    validation_input_batches,\n",
        "    validation_label_batches,\n",
        "    optimizer,\n",
        "    model,\n",
        "    list(range(possible_labels))\n",
        ")\n"
      ],
      "metadata": {
        "id": "hhmsgNFE3p1P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "502cf3e1-9586-4c12-a953-4add1bf1c78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:05<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss: 0.49792732407407064\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/41 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a4cff0c4c27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trained_model = training_loop(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_input_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_label_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-224f7600a12d>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(num_epochs, train_features, train_labels, dev_features, dev_labels, optimizer, model, possible_labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdev_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmacro_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2v0Rso2gPXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}